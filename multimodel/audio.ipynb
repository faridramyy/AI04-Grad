{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "#the virtual environment is new_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../datasets/CREMA-D/MELD-master/data/MELD/train_sent_emo.csv', delimiter=',', nrows = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>also I was the point person on my company’s tr...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:16,059</td>\n",
       "      <td>00:16:21,731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>You must’ve had your hands full.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:21,940</td>\n",
       "      <td>00:16:23,442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:23,442</td>\n",
       "      <td>00:16:26,389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>So let’s talk a little bit about your duties.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:26,820</td>\n",
       "      <td>00:16:29,572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:34,452</td>\n",
       "      <td>00:16:40,917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr No.                                          Utterance          Speaker  \\\n",
       "0       1  also I was the point person on my company’s tr...         Chandler   \n",
       "1       2                   You must’ve had your hands full.  The Interviewer   \n",
       "2       3                            That I did. That I did.         Chandler   \n",
       "3       4      So let’s talk a little bit about your duties.  The Interviewer   \n",
       "4       5                             My duties?  All right.         Chandler   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
       "0   neutral   neutral            0             0       8       21   \n",
       "1   neutral   neutral            0             1       8       21   \n",
       "2   neutral   neutral            0             2       8       21   \n",
       "3   neutral   neutral            0             3       8       21   \n",
       "4  surprise  positive            0             4       8       21   \n",
       "\n",
       "      StartTime       EndTime  \n",
       "0  00:16:16,059  00:16:21,731  \n",
       "1  00:16:21,940  00:16:23,442  \n",
       "2  00:16:23,442  00:16:26,389  \n",
       "3  00:16:26,820  00:16:29,572  \n",
       "4  00:16:34,452  00:16:40,917  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>also I was the point person on my company’s tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You must’ve had your hands full.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That I did. That I did.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So let’s talk a little bit about your duties.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My duties?  All right.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Utterance\n",
       "0  also I was the point person on my company’s tr...\n",
       "1                   You must’ve had your hands full.\n",
       "2                            That I did. That I did.\n",
       "3      So let’s talk a little bit about your duties.\n",
       "4                             My duties?  All right."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df=df['Utterance']\n",
    "text_df=pd.DataFrame(text_df)\n",
    "text_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize, pad, and truncate\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "text_df['input_ids'] = text_df['Utterance'].apply(lambda x: preprocess(x)['input_ids'][0])\n",
    "text_df['attention_mask'] = text_df['Utterance'].apply(lambda x: preprocess(x)['attention_mask'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self,input_ids,attention_masks):\n",
    "        self.input_ids=input_ids\n",
    "        self.attention_masks=attention_masks\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self,idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_masks[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "dataset=TextDataset(text_df['input_ids'].to_list(),text_df['attention_mask'].to_list())\n",
    "train_loader = DataLoader(dataset, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # Check PyTorch version\n",
    "print(torch.cuda.is_available())  # Check if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9989 [00:00<?, ?it/s]C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_18816\\707734850.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_18816\\707734850.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_masks[idx], dtype=torch.long),\n",
      "100%|██████████| 9989/9989 [29:04<00:00,  5.73it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load model and force it to use the CPU\n",
    "model = BertModel.from_pretrained('bert-base-uncased', num_labels=7).to('cpu')\n",
    "model.eval()\n",
    "\n",
    "from tqdm import tqdm\n",
    "encodings = []\n",
    "\n",
    "# Force DataLoader batches to remain on the CPU\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],  # Default is CPU\n",
    "            attention_mask=batch['attention_mask'],  # Default is CPU\n",
    "        )\n",
    "        encoding = outputs.last_hidden_state\n",
    "        encodings.append(encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3271, -0.0116, -0.0164,  ..., -0.1454,  0.4163,  0.1177],\n",
       "         [ 0.7810, -0.1910,  0.3150,  ..., -0.3541,  0.9698, -0.2329],\n",
       "         [-0.2891, -0.7291,  0.1984,  ...,  0.8826,  0.8325, -0.5647],\n",
       "         ...,\n",
       "         [ 0.2801,  0.3044,  0.5951,  ...,  0.1329,  0.4678, -0.0913],\n",
       "         [-0.2380, -0.1202,  0.5666,  ...,  0.6372,  0.5232, -0.1867],\n",
       "         [ 0.1859,  0.2200,  0.6492,  ...,  0.0793,  0.3894, -0.0810]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[9987]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encodings=[t.flatten() for t in encodings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_encodings_np = [encoding.cpu().numpy() for encoding in text_encodings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bert_encodings_np[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('text_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(bert_encodings_np, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_embeddings.pkl', 'rb') as f:\n",
    "    loaded_array_list = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_masks[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Prepare labels (map emotion labels to numbers)\n",
    "label_mapping = {'neutral': 0, 'joy': 1, 'sadness': 2, 'anger': 3, 'surprise': 4, 'fear': 5, 'disgust': 6}\n",
    "df['label'] = df['Emotion'].map(label_mapping)\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "dataset = TextDataset(\n",
    "    text_df['input_ids'].to_list(),\n",
    "    text_df['attention_mask'].to_list(),\n",
    "    df['label'].to_list()\n",
    ")\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pre-trained BERT with classification head\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=7).to('cpu')\n",
    "\n",
    "# Define optimizer and loss function\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss, filepath=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved at epoch {epoch + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_18816\\1479609361.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_18816\\1479609361.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_masks[idx], dtype=torch.long),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 1\n",
      "Epoch 1/3, Loss: 1.137200417610022\n",
      "Checkpoint saved at epoch 2\n",
      "Epoch 2/3, Loss: 0.9028418294537943\n",
      "Checkpoint saved at epoch 3\n",
      "Epoch 3/3, Loss: 0.6272407133167925\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "checkpoint_path = \"bert_emotion_checkpoint.pth\"\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'].to('cpu'),\n",
    "            attention_mask=batch['attention_mask'].to('cpu'),\n",
    "            labels=batch['labels'].to('cpu'),\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save checkpoint at the end of each epoch\n",
    "    save_checkpoint(epoch, model, optimizer, total_loss / len(train_loader), checkpoint_path)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "\n",
    "\n",
    "#testing the model \n",
    "\n",
    "\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_csv('../datasets/CREMA-D/MELD-master/data/MELD/test_sent_emo.csv', delimiter=',')\n",
    "\n",
    "# Preprocess the text data (like you did for training data)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "test_df['input_ids'] = test_df['Utterance'].apply(lambda x: preprocess(x)['input_ids'][0])\n",
    "test_df['attention_mask'] = test_df['Utterance'].apply(lambda x: preprocess(x)['attention_mask'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map emotions to numbers (same as training)\n",
    "label_mapping = {'neutral': 0, 'joy': 1, 'sadness': 2, 'anger': 3, 'surprise': 4, 'fear': 5, 'disgust': 6}\n",
    "test_df['label'] = test_df['Emotion'].map(label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "\n",
    "#audio part :\n",
    "\n",
    "\n",
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the audio directory in MELD dataset\n",
    "audio_dir = \"../datasets/MELD/audio\"\n",
    "\n",
    "# List all audio files\n",
    "audio_files = os.listdir(audio_dir)\n",
    "\n",
    "# Check the number of audio files\n",
    "print(f\"Number of audio files: {len(audio_files)}\")\n",
    "\n",
    "# Display the first 10 files\n",
    "print(\"First 10 audio files:\")\n",
    "print(audio_files[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
