{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import time\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI API Key securely from .env file\n",
    "def load_api_key() -> None:\n",
    "    \"\"\"Loads the OpenAI API key from .env file.\"\"\"\n",
    "    env_path = \"../models/key.env\"\n",
    "    load_dotenv(env_path)\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"‚ùå OpenAI API Key not found. Make sure it's in the .env file.\")\n",
    "    \n",
    "    openai.api_key = api_key\n",
    "    \n",
    "    print(\"‚úÖ OpenAI API Key loaded successfully!\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced dataset loading with validation\n",
    "def load_dataset() -> List[Dict]:\n",
    "    \"\"\"Loads and validates the JSON dataset for emotion-based scenarios.\"\"\"\n",
    "    dataset_path = \"../datasets/game.json\"\n",
    "\n",
    "    try:\n",
    "        with open(dataset_path, \"r\") as file:\n",
    "            dataset = json.load(file)\n",
    "        \n",
    "        # Validate dataset structure\n",
    "        required_keys = {\"scenario\", \"responses\", \"correct_response\", \"difficulty\"}\n",
    "        for intent in dataset.get(\"intents\", []):\n",
    "            if not all(key in intent for key in required_keys):\n",
    "                raise ValueError(f\"‚ùå Invalid dataset structure. Missing required keys in intent: {intent}\")\n",
    "            \n",
    "            # Validate responses\n",
    "            for response in intent[\"responses\"]:\n",
    "                if \"text\" not in response or \"reward\" not in response:\n",
    "                    raise ValueError(\"‚ùå Each response must have 'text' and 'reward' fields\")\n",
    "        \n",
    "        print(f\"‚úÖ Dataset loaded successfully with {len(dataset['intents'])} scenarios!\")\n",
    "        return dataset[\"intents\"]\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"‚ùå Dataset file '{dataset_path}' not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"‚ùå Failed to parse JSON file '{dataset_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stress measurement metrics\n",
    "class StressMetrics:\n",
    "    \"\"\"Tracks and calculates stress-related metrics during interactions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stress_level = 0\n",
    "        self.stress_history = []\n",
    "        self.response_times = []\n",
    "        self.incorrect_choices = 0\n",
    "        self.correct_choices = 0\n",
    "    \n",
    "    def update_stress(self, reward: float, response_time: float) -> None:\n",
    "        \"\"\"Updates stress metrics based on the interaction.\"\"\"\n",
    "        stress_change = 0\n",
    "        \n",
    "        if reward < 0:\n",
    "            self.incorrect_choices += 1\n",
    "            stress_change = 0.2  # Stress increases with incorrect choices\n",
    "        else:\n",
    "            self.correct_choices += 1\n",
    "            stress_change = -0.1  # Stress decreases with correct choices\n",
    "        \n",
    "        # Longer response times increase stress\n",
    "        stress_change += min(response_time / 10, 0.5)\n",
    "        \n",
    "        self.stress_level = max(0, min(1, self.stress_level + stress_change))\n",
    "        self.stress_history.append(self.stress_level)\n",
    "        self.response_times.append(response_time)\n",
    "    \n",
    "    def get_stress_score(self) -> float:\n",
    "        \"\"\"Calculates a normalized stress score (0-1).\"\"\"\n",
    "        if not self.stress_history:\n",
    "            return 0\n",
    "        \n",
    "        # Weighted average with more emphasis on recent stress\n",
    "        recent_stress = self.stress_history[-min(5, len(self.stress_history)):]\n",
    "        return sum(recent_stress) / len(recent_stress)\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Returns a summary of stress metrics.\"\"\"\n",
    "        return {\n",
    "            \"current_stress\": self.stress_level,\n",
    "            \"average_stress\": sum(self.stress_history) / len(self.stress_history) if self.stress_history else 0,\n",
    "            \"correct_choices\": self.correct_choices,\n",
    "            \"incorrect_choices\": self.incorrect_choices,\n",
    "            \"average_response_time\": sum(self.response_times) / len(self.response_times) if self.response_times else 0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enhanced Emotion-Based RL Environment\n",
    "class EmotionGameEnv(gym.Env):\n",
    "    \"\"\"Custom Gym environment for Emotion-Based RL with improved features.\"\"\"\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EmotionGameEnv, self).__init__()\n",
    "\n",
    "        # Load emotion dataset\n",
    "        self.dataset = load_dataset()\n",
    "        self.stress_metrics = StressMetrics()\n",
    "        self.interaction_history = []\n",
    "        self.current_episode = 0\n",
    "\n",
    "        # Define state and action spaces\n",
    "        self.emotions = [\"sad\", \"angry\", \"happy\", \"stress\", \"neutral\", \"fear\", \"anxious\"]\n",
    "        self.difficulty_levels = [\"easy\", \"medium\", \"hard\"]\n",
    "        \n",
    "        # State includes emotion and difficulty\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"emotion\": spaces.Discrete(len(self.emotions)),\n",
    "            \"difficulty\": spaces.Discrete(len(self.difficulty_levels))\n",
    "        })\n",
    "        \n",
    "        self.action_space = spaces.Discrete(4)  # 4 choices per scenario\n",
    "        self.max_episode_length = 10  # Maximum interactions per episode\n",
    "\n",
    "        self.state = None\n",
    "        self.current_scenario = None\n",
    "        self.episode_step = 0\n",
    "        self.available_scenarios = self._categorize_scenarios()\n",
    "\n",
    "    def _categorize_scenarios(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Categorizes scenarios by emotion and difficulty.\"\"\"\n",
    "        categorized = defaultdict(list)\n",
    "        for scenario in self.dataset:\n",
    "            categorized[scenario.get(\"emotion\", \"neutral\")].append(scenario)\n",
    "        return categorized\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment for a new episode.\"\"\"\n",
    "        self.current_episode += 1\n",
    "        self.episode_step = 0\n",
    "        \n",
    "        # Select random emotion and difficulty\n",
    "        emotion = random.choice(self.emotions)\n",
    "        difficulty = random.choice(self.difficulty_levels)\n",
    "        \n",
    "        # Filter scenarios by emotion and difficulty\n",
    "        possible_scenarios = [\n",
    "            s for s in self.available_scenarios.get(emotion, []) \n",
    "            if s.get(\"difficulty\", \"easy\") == difficulty\n",
    "        ]\n",
    "        \n",
    "        if not possible_scenarios:\n",
    "            possible_scenarios = self.dataset  # Fallback to all scenarios\n",
    "        \n",
    "        self.current_scenario = random.choice(possible_scenarios)\n",
    "        self.state = {\n",
    "            \"emotion\": self.emotions.index(emotion),\n",
    "            \"difficulty\": self.difficulty_levels.index(difficulty)\n",
    "        }\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "    def step(self, action: int) -> Tuple[Dict, float, bool, Dict]:\n",
    "        \"\"\"Executes one step in the environment.\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.episode_step += 1\n",
    "        \n",
    "        try:\n",
    "            responses = self.current_scenario[\"responses\"]\n",
    "            if action >= len(responses):\n",
    "                reward = -1  # Penalize invalid actions\n",
    "                response_data = {\"text\": \"Invalid choice\", \"reward\": reward}\n",
    "            else:\n",
    "                response_data = responses[action]\n",
    "                reward = response_data[\"reward\"]\n",
    "                \n",
    "                # Adjust reward based on difficulty\n",
    "                difficulty = self.difficulty_levels[self.state[\"difficulty\"]]\n",
    "                if difficulty == \"hard\":\n",
    "                    reward *= 1.5\n",
    "                elif difficulty == \"easy\":\n",
    "                    reward *= 0.8\n",
    "        except (IndexError, KeyError):\n",
    "            reward = -1  # Default penalty for errors\n",
    "            response_data = {\"text\": \"Error in scenario\", \"reward\": reward}\n",
    "        \n",
    "        # Calculate response time and update stress metrics\n",
    "        response_time = time.time() - start_time\n",
    "        self.stress_metrics.update_stress(reward, response_time)\n",
    "        \n",
    "        # Track interaction history\n",
    "        self.interaction_history.append({\n",
    "            \"episode\": self.current_episode,\n",
    "            \"step\": self.episode_step,\n",
    "            \"emotion\": self.emotions[self.state[\"emotion\"]],\n",
    "            \"scenario\": self.current_scenario[\"scenario\"],\n",
    "            \"action\": action,\n",
    "            \"response\": response_data[\"text\"],\n",
    "            \"reward\": reward,\n",
    "            \"response_time\": response_time,\n",
    "            \"stress_level\": self.stress_metrics.stress_level\n",
    "        })\n",
    "        \n",
    "        # Check if episode should end\n",
    "        done = self.episode_step >= self.max_episode_length\n",
    "        if reward < -0.5:  # End early on very bad choices\n",
    "            done = True\n",
    "        \n",
    "        info = {\n",
    "            \"scenario\": self.current_scenario[\"scenario\"],\n",
    "            \"response\": response_data[\"text\"],\n",
    "            \"stress_level\": self.stress_metrics.stress_level,\n",
    "            \"correct_response\": self.current_scenario.get(\"correct_response\", \"\"),\n",
    "            \"difficulty\": self.difficulty_levels[self.state[\"difficulty\"]]\n",
    "        }\n",
    "        \n",
    "        # Get next state (same emotion but may change difficulty)\n",
    "        next_state = {\n",
    "            \"emotion\": self.state[\"emotion\"],\n",
    "            \"difficulty\": random.choice([self.state[\"difficulty\"]] * 3 + \n",
    "                                      [max(0, self.state[\"difficulty\"]-1), \n",
    "                                      min(len(self.difficulty_levels)-1, self.state[\"difficulty\"]+1)])\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Renders the current state of the environment.\"\"\"\n",
    "        if mode == 'human':\n",
    "            print(f\"\\nCurrent Emotion: {self.emotions[self.state['emotion']]}\")\n",
    "            print(f\"Difficulty: {self.difficulty_levels[self.state['difficulty']]}\")\n",
    "            print(f\"Scenario: {self.current_scenario['scenario']}\")\n",
    "            print(f\"Current Stress Level: {self.stress_metrics.stress_level:.2f}\")\n",
    "            print(f\"Episode Step: {self.episode_step}/{self.max_episode_length}\")\n",
    "\n",
    "    def get_history(self) -> List[Dict]:\n",
    "        \"\"\"Returns the interaction history.\"\"\"\n",
    "        return self.interaction_history\n",
    "\n",
    "    def get_stress_summary(self) -> Dict:\n",
    "        \"\"\"Returns stress metrics summary.\"\"\"\n",
    "        return self.stress_metrics.get_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enhanced GPT response generator with therapy techniques\n",
    "def get_therapy_response(emotion: str, scenario: str, stress_level: float, history: List[Dict]) -> str:\n",
    "    \"\"\"Generates a therapeutic response using GPT with context-aware techniques.\"\"\"\n",
    "    # Build context from history\n",
    "    recent_history = history[-3:] if history else []\n",
    "    context = \"\\n\".join(\n",
    "        f\"Previous interaction: When feeling {h['emotion']} in situation '{h['scenario']}', \"\n",
    "        f\"the response was '{h['response']}' which resulted in reward {h['reward']:.1f} \"\n",
    "        f\"and stress level {h['stress_level']:.2f}.\"\n",
    "        for h in recent_history\n",
    "    )\n",
    "    \n",
    "    # Select therapy approach based on emotion and stress\n",
    "    therapy_approaches = {\n",
    "        \"stress\": \"cognitive behavioral therapy techniques\",\n",
    "        \"anxious\": \"grounding exercises and mindfulness\",\n",
    "        \"angry\": \"anger management strategies\",\n",
    "        \"sad\": \"positive reframing and self-compassion exercises\",\n",
    "        \"happy\": \"reinforcement of positive behaviors\",\n",
    "        \"fear\": \"exposure therapy principles\",\n",
    "        \"neutral\": \"general counseling techniques\"\n",
    "    }\n",
    "    \n",
    "    approach = therapy_approaches.get(emotion, \"general counseling techniques\")\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Act as an AI therapist helping a user manage emotions. The user is currently feeling {emotion} \"\n",
    "        f\"(stress level: {stress_level:.2f}/1.0) in this situation: '{scenario}'. \"\n",
    "        f\"Use {approach} to provide a helpful response. Keep it concise (1-2 sentences) and therapeutic. \"\n",
    "        f\"Here's recent context:\\n{context}\\n\\nTherapeutic response:\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Using more cost-effective model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a compassionate AI therapist trained in CBT, DBT, and mindfulness techniques.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        return f\"I'm having trouble generating a response right now. Please try again later. Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enhanced RL Model Training with Callbacks\n",
    "def train_rl_model(env: EmotionGameEnv, total_timesteps: int = 20000) -> PPO:\n",
    "    \"\"\"Trains the RL model with improved settings and callbacks.\"\"\"\n",
    "    # Environment check\n",
    "    check_env(env)\n",
    "    \n",
    "    # Callback for early stopping and model saving\n",
    "    eval_callback = EvalCallback(\n",
    "        env,\n",
    "        callback_on_new_best=StopTrainingOnRewardThreshold(reward_threshold=5.0, verbose=1),\n",
    "        verbose=1,\n",
    "        eval_freq=1000,\n",
    "        best_model_save_path=\"./best_models/\"\n",
    "    )\n",
    "    \n",
    "    model = PPO(\n",
    "        \"MultiInputPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        learning_rate=0.0003,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=0.01,\n",
    "        tensorboard_log=\"./tensorboard_logs/\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nüéØ Training PPO model with enhanced settings...\\n\")\n",
    "        model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=eval_callback,\n",
    "            tb_log_name=\"emotion_rl\"\n",
    "        )\n",
    "        model.save(\"emotional_rl_agent_enhanced\")\n",
    "        print(\"‚úÖ Model training completed and saved as 'emotional_rl_agent_enhanced'.\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"‚è∏ Training interrupted. Saving model...\")\n",
    "        model.save(\"emotional_rl_agent_enhanced_interrupted\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Comprehensive Evaluation Function\n",
    "def evaluate_model(env: EmotionGameEnv, model_path: str = \"emotional_rl_agent_enhanced\", num_tests: int = 5) -> None:\n",
    "    \"\"\"Evaluates the model with detailed analysis and therapeutic feedback.\"\"\"\n",
    "    print(\"\\nüìä Starting comprehensive evaluation...\")\n",
    "    \n",
    "    try:\n",
    "        model = PPO.load(model_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Model file '{model_path}' not found. Please train the model first.\")\n",
    "        return\n",
    "    \n",
    "    for test_num in range(1, num_tests + 1):\n",
    "        print(f\"\\n=== Test Case {test_num}/{num_tests} ===\")\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        episode_steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            env.render()\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Get therapeutic response\n",
    "            emotion = env.emotions[obs[\"emotion\"]]\n",
    "            gpt_response = get_therapy_response(\n",
    "                emotion,\n",
    "                info[\"scenario\"],\n",
    "                env.stress_metrics.stress_level,\n",
    "                env.get_history()\n",
    "            )\n",
    "            \n",
    "            total_reward += reward\n",
    "            episode_steps += 1\n",
    "            \n",
    "            print(f\"\\nStep {episode_steps}:\")\n",
    "            print(f\"Emotion: {emotion}\")\n",
    "            print(f\"Difficulty: {info['difficulty']}\")\n",
    "            print(f\"Scenario: {info['scenario']}\")\n",
    "            print(f\"Action Taken: {action}\")\n",
    "            print(f\"Response: {info['response']}\")\n",
    "            print(f\"Reward: {reward:.2f} (Total: {total_reward:.2f})\")\n",
    "            print(f\"Stress Level: {env.stress_metrics.stress_level:.2f}\")\n",
    "            print(f\"Therapeutic Feedback: {gpt_response}\")\n",
    "            \n",
    "            if done:\n",
    "                print(\"\\nEpisode completed!\")\n",
    "                print(f\"Total Reward: {total_reward:.2f}\")\n",
    "                print(f\"Steps Taken: {episode_steps}\")\n",
    "                print(f\"Final Stress Level: {env.stress_metrics.stress_level:.2f}\")\n",
    "                \n",
    "                # Show stress metrics summary\n",
    "                stress_summary = env.stress_metrics.get_summary()\n",
    "                print(\"\\nStress Metrics Summary:\")\n",
    "                for k, v in stress_summary.items():\n",
    "                    print(f\"- {k.replace('_', ' ').title()}: {v if isinstance(v, int) else f'{v:.2f}'}\")\n",
    "                \n",
    "                # Reset stress metrics for next episode\n",
    "                env.stress_metrics = StressMetrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main Execution Function with Enhanced Features\n",
    "def main():\n",
    "    \"\"\"Main function with improved setup and error handling.\"\"\"\n",
    "    print(\"üöÄ Starting Enhanced Emotion-Based RL Therapy System...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load OpenAI API Key\n",
    "        load_api_key()\n",
    "        \n",
    "        # Initialize the environment\n",
    "        env = EmotionGameEnv()\n",
    "        \n",
    "        # Train or load the RL model\n",
    "        train_new_model = input(\"Train new model? (y/n): \").lower() == 'y'\n",
    "        \n",
    "        if train_new_model:\n",
    "            timesteps = int(input(\"Enter training timesteps (default: 20000): \") or 20000)\n",
    "            model = train_rl_model(env, total_timesteps=timesteps)\n",
    "        else:\n",
    "            model_path = input(\"Enter model path (default: emotional_rl_agent_enhanced): \") or \"emotional_rl_agent_enhanced\"\n",
    "            model = PPO.load(model_path)\n",
    "        \n",
    "        # Evaluate the model with therapeutic feedback\n",
    "        num_tests = int(input(\"Enter number of test cases (default: 5): \") or 5)\n",
    "        evaluate_model(env, num_tests=num_tests)\n",
    "        \n",
    "        # Save interaction history\n",
    "        save_history = input(\"Save interaction history? (y/n): \").lower() == 'y'\n",
    "        if save_history:\n",
    "            history = env.get_history()\n",
    "            with open(\"therapy_session_history.json\", \"w\") as f:\n",
    "                json.dump(history, f, indent=2)\n",
    "            print(\"‚úÖ Session history saved to 'therapy_session_history.json'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        print(\"\\nüèÅ Session completed. Wishing you emotional well-being!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
