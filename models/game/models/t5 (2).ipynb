{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Adaptive Emotion RL Game (FLAN-T5)...\n",
      "\n",
      "‚úÖ Detected emotions: ['happy']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837f49396e0541e1b021756e08da90d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Share the news with close friends and family.', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba35462d0304464b77a1bb93dc53dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Treat yourself to something you enjoy.', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0da6d2201f343bfbfc935c8b8247cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Downplay your achievement and not share it with anyone.', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41852e4159264c008a58f933dd7e0f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Compare yourself to others and feel like it‚Äôs not good enough.', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24e4a89f33d4c738755e65502322675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# ‚úÖ Load seed dataset\n",
    "def load_seed_dataset():\n",
    "    path = \"../datasets/game.json\"\n",
    "    try:\n",
    "        with open(path, \"r\") as file:\n",
    "            return json.load(file)[\"intents\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Dataset load error: {e}\")\n",
    "        return []\n",
    "      \n",
    "def generate_followup_scenario_with_flan(prev_scenario, chosen_option, model, tokenizer):\n",
    "    print(\"üîÅ Generating a follow-up scenario based on the user's choice...\\n\")\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are a therapeutic AI that creates emotional decision-making game scenarios.\\n\"\n",
    "        f\"Here is what happened previously:\\n\"\n",
    "        f\"Scenario: {prev_scenario['scenario']}\\n\"\n",
    "        f\"User chose: {chosen_option}\\n\"\n",
    "        f\"Now, generate a new follow-up scenario as a JSON object with:\\n\"\n",
    "        f\"- 'scenario': a stressful or emotional situation\\n\"\n",
    "        f\"- 'emotion': one of ['happy', 'sad', 'stressed', 'angry', 'anxious', 'confused']\\n\"\n",
    "        f\"- 'difficulty': 'easy', 'medium', or 'hard'\\n\"\n",
    "        f\"- 'responses': list of 4 responses, each with 'option' and 'reward' (-3 to +3)\\n\"\n",
    "        f\"- 'best_choice_index': the index of the most appropriate response\\n\"\n",
    "        f\"Only return the JSON object.\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.8\n",
    "        )\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    try:\n",
    "        new_scenario = json.loads(decoded)\n",
    "        print(\"‚úÖ Follow-up scenario generated successfully.\")\n",
    "        return new_scenario\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"‚ùå Failed to parse JSON. Raw output:\")\n",
    "        print(decoded)\n",
    "        return None\n",
    "\n",
    "# ‚úÖ Emotion-based Gym Environment\n",
    "class EmotionGameEnv(gym.Env):\n",
    "    def __init__(self, combined_data):\n",
    "        super().__init__()\n",
    "        self.dataset = combined_data\n",
    "        self.emotions = sorted(list(set(s[\"emotion\"] for s in self.dataset)))\n",
    "        print(f\"‚úÖ Detected emotions: {self.emotions}\")\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Discrete(len(self.emotions))\n",
    "\n",
    "        self.stress = 0.5\n",
    "        self.stress_history = []\n",
    "        self.state = None\n",
    "        self.current_scenario = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.stress = 0.5\n",
    "        self.stress_history = [self.stress]\n",
    "        self.current_scenario = random.choice(self.dataset)\n",
    "        self.state = self.current_scenario[\"emotion\"]\n",
    "        return self.emotions.index(self.state)\n",
    "\n",
    "    def step(self, action):\n",
    "        try:\n",
    "            reward = self.current_scenario[\"responses\"][action][\"reward\"]\n",
    "        except:\n",
    "            reward = 0\n",
    "\n",
    "        self.stress = min(1.0, max(0.0, self.stress - reward * 0.05 + random.uniform(-0.02, 0.02)))\n",
    "        self.stress_history.append(self.stress)\n",
    "\n",
    "        done = True\n",
    "        info = {\n",
    "            \"scenario\": self.current_scenario[\"scenario\"],\n",
    "            \"stress\": self.stress\n",
    "        }\n",
    "        return self.emotions.index(self.state), reward, done, info\n",
    "\n",
    "# ‚úÖ Train RL model\n",
    "def train_rl_model(env, total_timesteps=5000):\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "    print(\"üéØ Training RL model...\")\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    model.save(\"emotional_rl_agent\")\n",
    "    print(\"‚úÖ Model saved as 'emotional_rl_agent'.\")\n",
    "    return model\n",
    "\n",
    "# ‚úÖ Evaluate and plot stress\n",
    "def evaluate_model(env, model_path=\"emotional_rl_agent\", num_tests=5):\n",
    "    model = PPO.load(model_path)\n",
    "    print(\"\\nüìä Evaluating model...\\n\")\n",
    "    for _ in range(num_tests):\n",
    "        obs = env.reset()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        print(f\"\\nüé≠ Emotion: {env.state}\")\n",
    "        print(f\"üìú Scenario: {info['scenario']}\")\n",
    "        print(f\"‚úÖ Action Taken: {action}\")\n",
    "        print(f\"üèÜ Reward: {reward}\")\n",
    "        print(f\"üìà Stress Level: {round(info['stress'], 2)}\")\n",
    "\n",
    "    plt.plot(env.stress_history)\n",
    "    plt.title(\"üìâ Stress Level Over Time\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Stress\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ‚úÖ Interactive button-based gameplay\n",
    "def adaptive_interactive_gameplay(initial_scenario, model, tokenizer, rounds=5):\n",
    "    current_scenario = initial_scenario\n",
    "    env = EmotionGameEnv([current_scenario])  # Start with single scenario\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def play_round(round_index):\n",
    "        nonlocal current_scenario  # ‚úÖ Declare nonlocal once at the start of the round\n",
    "        env.dataset = [current_scenario]\n",
    "        env.reset()\n",
    "\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(f\"\\nüé≠ Emotion: {current_scenario['emotion']}\")\n",
    "            print(f\"üìú Scenario: {current_scenario['scenario']}\")\n",
    "\n",
    "        buttons = []\n",
    "\n",
    "        def handle_choice(i):\n",
    "            def on_click(btn):\n",
    "                nonlocal current_scenario  # ‚úÖ Must be declared before any usage inside this function\n",
    "\n",
    "                user_choice = current_scenario[\"responses\"][i][\"option\"]\n",
    "                _, reward, _, info = env.step(i)\n",
    "\n",
    "                with output:\n",
    "                    clear_output()\n",
    "                    print(f\"‚úÖ You chose: {user_choice}\")\n",
    "                    print(f\"üèÜ Reward: {reward}\")\n",
    "                    print(f\"üìâ New Stress Level: {round(info['stress'], 2)}\")\n",
    "                    plt.plot(env.stress_history)\n",
    "                    plt.title(\"üìâ Stress Level\")\n",
    "                    plt.xlabel(\"Step\")\n",
    "                    plt.ylabel(\"Stress\")\n",
    "                    plt.grid(True)\n",
    "                    plt.show()\n",
    "\n",
    "                if round_index + 1 < rounds:\n",
    "                    new_scenario = generate_followup_scenario_with_flan(current_scenario, user_choice, model, tokenizer)\n",
    "                    if new_scenario:\n",
    "                        current_scenario = new_scenario\n",
    "                        next_button = widgets.Button(description=\"‚û° Next Scenario\")\n",
    "                        next_button.on_click(lambda b: play_round(round_index + 1))\n",
    "                        display(next_button)\n",
    "                    else:\n",
    "                        print(\"‚ùå Failed to generate next scenario.\")\n",
    "                else:\n",
    "                    print(\"\\nüéâ Game session complete.\")\n",
    "            return on_click\n",
    "\n",
    "        for i, resp in enumerate(current_scenario[\"responses\"]):\n",
    "            btn = widgets.Button(description=resp['option'])\n",
    "            btn.on_click(handle_choice(i))\n",
    "            buttons.append(btn)\n",
    "\n",
    "        display(*buttons)\n",
    "        display(output)\n",
    "\n",
    "    play_round(0)\n",
    "\n",
    "\n",
    "# ‚úÖ Main execution\n",
    "def main():\n",
    "    print(\"üöÄ Adaptive Emotion RL Game (FLAN-T5)...\\n\")\n",
    "    seed_data = load_seed_dataset()\n",
    "    if not seed_data:\n",
    "        print(\"‚ùå No seed data found.\")\n",
    "        return\n",
    "\n",
    "    model_name = \"google/flan-t5-small\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    first_scenario = random.choice(seed_data)\n",
    "    adaptive_interactive_gameplay(first_scenario, model, tokenizer, rounds=5)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Train RL model and save as .zip and .pkl\n",
    "def train_rl_model(env, total_timesteps=5000):\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "    print(\"üéØ Training RL model...\")\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "    # Standard SB3 save\n",
    "    model.save(\"emotional_rl_agent\")\n",
    "    print(\"‚úÖ Model saved as 'emotional_rl_agent.zip'.\")\n",
    "\n",
    "    # Optional: Save as .pkl\n",
    "    import joblib\n",
    "    joblib.dump(model, \"emotional_rl_agent.pkl\")\n",
    "    print(\"‚úÖ Model also saved as 'emotional_rl_agent.pkl'.\")\n",
    "\n",
    "    return model\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fer_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
