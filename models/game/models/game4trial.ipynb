{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "550e3b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\anaconda3\\envs\\fer_pytorch\\lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.4 when it was built against 1.14.2, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400c8a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gemini API Key loaded successfully!\n",
      "✅ Dataset loaded successfully with 79 scenarios!\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./tensorboard_logs/PPO_1\n",
      "Eval num_timesteps=1000, episode_reward=10.80 +/- 17.29\n",
      "Episode length: 4.20 +/- 3.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 10.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\anaconda3\\envs\\fer_pytorch\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Stopping training because the mean reward 10.80  is above the threshold 5.0\n",
      "\n",
      "📊 Evaluation Started\n",
      "\n",
      "=== Test Case 1 ===\n",
      "\n",
      "Emotion: angry, Difficulty: easy, Scenario: You recently lost something or someone important to you. How do you cope?, Stress Level: 0.40\n",
      "\n",
      "Therapeutic Feedback: ⚠️ Error from Gemini: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "Emotion: angry, Difficulty: easy, Scenario: You recently lost something or someone important to you. How do you cope?, Stress Level: 0.30\n",
      "\n",
      "Therapeutic Feedback: ⚠️ Error from Gemini: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "Emotion: angry, Difficulty: easy, Scenario: You recently lost something or someone important to you. How do you cope?, Stress Level: 0.20\n",
      "\n",
      "Therapeutic Feedback: ⚠️ Error from Gemini: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "=== Test Case 2 ===\n",
      "\n",
      "Emotion: anxious, Difficulty: hard, Scenario: It’s the weekend, and you’re feeling lonely because all your friends are busy. What do you do?, Stress Level: 0.40\n",
      "\n",
      "Therapeutic Feedback: ⚠️ Error from Gemini: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "=== Test Case 3 ===\n",
      "\n",
      "Emotion: stress, Difficulty: easy, Scenario: It's a regular day. You feel neither good nor bad. How do you spend your time?, Stress Level: 0.60\n",
      "\n",
      "Therapeutic Feedback: ⚠️ Error from Gemini: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load Gemini API Key\n",
    "def load_api_key() -> None:\n",
    "    env_path = \"../models/key.env\"\n",
    "    load_dotenv(env_path)\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\") or \"AIzaSyCcE1d9GrpAGiQh3Xrqlhs5E_wx4oAZ4d8\"\n",
    "    if not api_key:\n",
    "        raise ValueError(\"❌ Gemini API Key not found.\")\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"✅ Gemini API Key loaded successfully!\")\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset() -> List[Dict]:\n",
    "    dataset_path = \"../datasets/game.json\"\n",
    "    try:\n",
    "        with open(dataset_path, \"r\") as file:\n",
    "            dataset = json.load(file)\n",
    "        if \"intents\" not in dataset:\n",
    "            raise ValueError(\"❌ Dataset must have an 'intents' key.\")\n",
    "        for intent in dataset[\"intents\"]:\n",
    "            if \"scenario\" not in intent or \"responses\" not in intent:\n",
    "                raise ValueError(f\"❌ Each intent must have 'scenario' and 'responses': {intent}\")\n",
    "            for response in intent[\"responses\"]:\n",
    "                if \"option\" not in response or \"reward\" not in response:\n",
    "                    raise ValueError(\"❌ Each response must have 'option' and 'reward'.\")\n",
    "        print(f\"✅ Dataset loaded successfully with {len(dataset['intents'])} scenarios!\")\n",
    "        return dataset[\"intents\"]\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"❌ Dataset file '{dataset_path}' not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"❌ Failed to parse JSON file '{dataset_path}'.\")\n",
    "\n",
    "# Stress tracking\n",
    "class StressMetrics:\n",
    "    def __init__(self):\n",
    "        self.stress_level = 0\n",
    "        self.stress_history = []\n",
    "        self.response_times = []\n",
    "        self.incorrect_choices = 0\n",
    "        self.correct_choices = 0\n",
    "\n",
    "    def update_stress(self, reward: float, response_time: float) -> None:\n",
    "        stress_change = 0\n",
    "        if reward < 0:\n",
    "            self.incorrect_choices += 1\n",
    "            stress_change = 0.2\n",
    "        else:\n",
    "            self.correct_choices += 1\n",
    "            stress_change = -0.1\n",
    "        stress_change += min(response_time / 10, 0.5)\n",
    "        self.stress_level = max(0, min(1, self.stress_level + stress_change))\n",
    "        self.stress_history.append(self.stress_level)\n",
    "        self.response_times.append(response_time)\n",
    "\n",
    "    def get_summary(self) -> Dict:\n",
    "        return {\n",
    "            \"current_stress\": self.stress_level,\n",
    "            \"average_stress\": sum(self.stress_history) / len(self.stress_history) if self.stress_history else 0,\n",
    "            \"correct_choices\": self.correct_choices,\n",
    "            \"incorrect_choices\": self.incorrect_choices,\n",
    "            \"average_response_time\": sum(self.response_times) / len(self.response_times) if self.response_times else 0\n",
    "        }\n",
    "\n",
    "# Environment\n",
    "class EmotionGameEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dataset = load_dataset()\n",
    "        self.stress_metrics = StressMetrics()\n",
    "        self.interaction_history = []\n",
    "        self.current_episode = 0\n",
    "        self.emotions = [\"sad\", \"angry\", \"happy\", \"stress\", \"neutral\", \"fear\", \"anxious\"]\n",
    "        self.difficulty_levels = [\"easy\", \"medium\", \"hard\"]\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"emotion\": spaces.Discrete(len(self.emotions)),\n",
    "            \"difficulty\": spaces.Discrete(len(self.difficulty_levels))\n",
    "        })\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.max_episode_length = 10\n",
    "        self.state = None\n",
    "        self.current_scenario = None\n",
    "        self.episode_step = 0\n",
    "        self.available_scenarios = self._categorize_scenarios()\n",
    "\n",
    "    def _categorize_scenarios(self) -> Dict[str, List[Dict]]:\n",
    "        categorized = defaultdict(list)\n",
    "        for scenario in self.dataset:\n",
    "            categorized[scenario.get(\"emotion\", \"neutral\")].append(scenario)\n",
    "        return categorized\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_episode += 1\n",
    "        self.episode_step = 0\n",
    "        emotion = random.choice(self.emotions)\n",
    "        difficulty = random.choice(self.difficulty_levels)\n",
    "        possible_scenarios = [\n",
    "            s for s in self.available_scenarios.get(emotion, [])\n",
    "            if s.get(\"difficulty\") == difficulty\n",
    "        ]\n",
    "        if not possible_scenarios:\n",
    "            possible_scenarios = self.dataset\n",
    "        self.current_scenario = random.choice(possible_scenarios)\n",
    "        self.state = {\n",
    "            \"emotion\": self.emotions.index(emotion),\n",
    "            \"difficulty\": self.difficulty_levels.index(difficulty)\n",
    "        }\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action: int) -> Tuple[Dict, float, bool, bool, Dict]:\n",
    "        start_time = time.time()\n",
    "        self.episode_step += 1\n",
    "        try:\n",
    "            responses = self.current_scenario[\"responses\"]\n",
    "            if action >= len(responses):\n",
    "                reward = -1\n",
    "                response_text = \"Invalid choice\"\n",
    "            else:\n",
    "                response_data = responses[action]\n",
    "                reward = response_data[\"reward\"]\n",
    "                response_text = response_data.get(\"text\", response_data.get(\"option\", \"\"))\n",
    "                difficulty = self.difficulty_levels[self.state[\"difficulty\"]]\n",
    "                if difficulty == \"hard\":\n",
    "                    reward *= 1.5\n",
    "                elif difficulty == \"easy\":\n",
    "                    reward *= 0.8\n",
    "        except Exception:\n",
    "            reward = -1\n",
    "            response_text = \"Error in scenario\"\n",
    "\n",
    "        response_time = time.time() - start_time\n",
    "        self.stress_metrics.update_stress(reward, response_time)\n",
    "\n",
    "        self.interaction_history.append({\n",
    "            \"episode\": self.current_episode,\n",
    "            \"step\": self.episode_step,\n",
    "            \"emotion\": self.emotions[self.state[\"emotion\"]],\n",
    "            \"scenario\": self.current_scenario[\"scenario\"],\n",
    "            \"action\": action,\n",
    "            \"response\": response_text,\n",
    "            \"reward\": reward,\n",
    "            \"response_time\": response_time,\n",
    "            \"stress_level\": self.stress_metrics.stress_level\n",
    "        })\n",
    "\n",
    "        done = self.episode_step >= self.max_episode_length or reward < -0.5\n",
    "\n",
    "        info = {\n",
    "            \"scenario\": self.current_scenario[\"scenario\"],\n",
    "            \"response\": response_text,\n",
    "            \"stress_level\": self.stress_metrics.stress_level,\n",
    "            \"correct_response\": self.current_scenario.get(\"correct_response\", \"\"),\n",
    "            \"difficulty\": self.current_scenario.get(\"difficulty\", self.difficulty_levels[self.state[\"difficulty\"]])\n",
    "        }\n",
    "\n",
    "        next_state = {\n",
    "            \"emotion\": self.state[\"emotion\"],\n",
    "            \"difficulty\": random.choice([\n",
    "                self.state[\"difficulty\"]] * 3 + [\n",
    "                max(0, self.state[\"difficulty\"] - 1),\n",
    "                min(len(self.difficulty_levels)-1, self.state[\"difficulty\"] + 1)\n",
    "            ])\n",
    "        }\n",
    "\n",
    "        return next_state, reward, done, False, info\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"\\nEmotion: {self.emotions[self.state['emotion']]}, \"\n",
    "              f\"Difficulty: {self.difficulty_levels[self.state['difficulty']]}, \"\n",
    "              f\"Scenario: {self.current_scenario['scenario']}, \"\n",
    "              f\"Stress Level: {self.stress_metrics.stress_level:.2f}\")\n",
    "\n",
    "    def get_history(self):\n",
    "        return self.interaction_history\n",
    "\n",
    "    def get_stress_summary(self):\n",
    "        return self.stress_metrics.get_summary()\n",
    "\n",
    "# Gemini-based therapy\n",
    "def get_therapy_response(emotion: str, scenario: str, stress_level: float, history: List[Dict]) -> str:\n",
    "    recent_history = history[-3:] if history else []\n",
    "    context = \"\\n\".join(\n",
    "        f\"Previous interaction: When feeling {h['emotion']} in situation '{h['scenario']}', \"\n",
    "        f\"the response was '{h['response']}' resulting in reward {h['reward']:.1f} and stress {h['stress_level']:.2f}.\"\n",
    "        for h in recent_history\n",
    "    )\n",
    "\n",
    "    therapy_approaches = {\n",
    "        \"stress\": \"cognitive behavioral therapy\",\n",
    "        \"anxious\": \"grounding and mindfulness\",\n",
    "        \"angry\": \"anger management\",\n",
    "        \"sad\": \"positive reframing\",\n",
    "        \"happy\": \"positive reinforcement\",\n",
    "        \"fear\": \"exposure therapy\",\n",
    "        \"neutral\": \"general counseling\"\n",
    "    }\n",
    "    approach = therapy_approaches.get(emotion, \"general counseling\")\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are a compassionate AI therapist trained in CBT, DBT, and mindfulness.\\n\"\n",
    "        f\"The user feels {emotion} (stress level {stress_level:.2f}) in scenario: '{scenario}'.\\n\"\n",
    "        f\"Use {approach} to help them.\\n\"\n",
    "        f\"Here is context:\\n{context}\\nTherapeutic response:\"\n",
    "    )\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"gemini-pro\")\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Error from Gemini: {e}\"\n",
    "\n",
    "# Training\n",
    "def train_rl_model(env: EmotionGameEnv, total_timesteps: int = 20000) -> PPO:\n",
    "    check_env(env)\n",
    "    eval_callback = EvalCallback(\n",
    "        env,\n",
    "        callback_on_new_best=StopTrainingOnRewardThreshold(reward_threshold=5.0, verbose=1),\n",
    "        verbose=1,\n",
    "        eval_freq=1000,\n",
    "        best_model_save_path=\"./best_models/\"\n",
    "    )\n",
    "    model = PPO(\"MultiInputPolicy\", env, verbose=1, learning_rate=0.0003, tensorboard_log=\"./tensorboard_logs/\")\n",
    "    model.learn(total_timesteps=total_timesteps, callback=eval_callback)\n",
    "    model.save(\"emotional_rl_agent_enhanced\")\n",
    "    return model\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(env: EmotionGameEnv, model_path: str = \"emotional_rl_agent_enhanced\", num_tests: int = 5):\n",
    "    print(\"\\n📊 Evaluation Started\")\n",
    "    try:\n",
    "        model = PPO.load(model_path)\n",
    "    except:\n",
    "        print(\"Model not found.\")\n",
    "        return\n",
    "\n",
    "    for test in range(num_tests):\n",
    "        print(f\"\\n=== Test Case {test+1} ===\")\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            emotion = env.emotions[obs[\"emotion\"]]\n",
    "            gpt_response = get_therapy_response(emotion, info[\"scenario\"], env.stress_metrics.stress_level, env.get_history())\n",
    "            print(f\"\\nTherapeutic Feedback: {gpt_response}\")\n",
    "\n",
    "# Entry point\n",
    "def main():\n",
    "    load_api_key()\n",
    "    env = EmotionGameEnv()\n",
    "    train_new = input(\"Train new model? (y/n): \").lower() == 'y'\n",
    "    if train_new:\n",
    "        model = train_rl_model(env)\n",
    "    else:\n",
    "        model = PPO.load(\"emotional_rl_agent_enhanced\")\n",
    "    evaluate_model(env, num_tests=3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fer_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
