{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6cfe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Emotion-Based RL & AI Therapy System - Training Only\n",
      "üîë Using Gemini API Key: AIzaS...wPI70\n",
      "\n",
      "üì¶ Training PPO model (default: 10000 timesteps)...\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\anaconda3\\envs\\whisper_env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Training PPO model with adaptive difficulty...\n",
      "\n",
      "Error generating scenario: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "\n",
      "‚è∏ Training interrupted. Saving model...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import requests\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from collections import deque\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Gemini API Configuration\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"AIzaSyA2rRBLcJx_e5g4d_fVOgG4q2Pf8ewPI70\")\n",
    "GEMINI_API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent\"\n",
    "\n",
    "# Constants\n",
    "DEFAULT_MODEL_PARAMS = {\n",
    "    \"policy\": \"MultiInputPolicy\",\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"n_steps\": 2048,\n",
    "    \"batch_size\": 64,\n",
    "    \"n_epochs\": 10,\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_range\": 0.2,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"verbose\": 1\n",
    "}\n",
    "\n",
    "class DifficultyAdjuster:\n",
    "    \"\"\"Handles dynamic difficulty adjustment based on performance.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.performance_window = deque(maxlen=10)  # Tracks last 10 performances\n",
    "        self.difficulty_levels = [\"easy\", \"medium\", \"hard\"]\n",
    "        self.current_difficulty_idx = 1  # Start with medium\n",
    "        \n",
    "    def update_difficulty(self, is_correct):\n",
    "        \"\"\"Adjust difficulty based on recent performance.\"\"\"\n",
    "        self.performance_window.append(1 if is_correct else 0)\n",
    "        \n",
    "        if len(self.performance_window) == self.performance_window.maxlen:\n",
    "            success_rate = sum(self.performance_window) / len(self.performance_window)\n",
    "            \n",
    "            if success_rate > 0.7:  # Doing well, increase difficulty\n",
    "                self.current_difficulty_idx = min(self.current_difficulty_idx + 1, len(self.difficulty_levels) - 1)\n",
    "            elif success_rate < 0.4:  # Struggling, decrease difficulty\n",
    "                self.current_difficulty_idx = max(self.current_difficulty_idx - 1, 0)\n",
    "                \n",
    "        return self.get_current_difficulty()\n",
    "    \n",
    "    def get_current_difficulty(self):\n",
    "        return self.difficulty_levels[self.current_difficulty_idx]\n",
    "\n",
    "class TrainingCallback(BaseCallback):\n",
    "    \"\"\"Custom callback for tracking training progress and adjusting parameters.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super(TrainingCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # Track episode rewards and lengths\n",
    "        if len(self.model.ep_info_buffer) > 0:\n",
    "            self.episode_rewards.extend([ep_info[\"r\"] for ep_info in self.model.ep_info_buffer if \"r\" in ep_info])\n",
    "            self.episode_lengths.extend([ep_info[\"l\"] for ep_info in self.model.ep_info_buffer if \"l\" in ep_info])\n",
    "            \n",
    "    def get_mean_reward(self):\n",
    "        return np.mean(self.episode_rewards) if self.episode_rewards else 0\n",
    "\n",
    "def generate_gemini_scenario(emotion, difficulty=\"medium\"):\n",
    "    \"\"\"Generates a scenario with choices and correct answer using Gemini API.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Generate a therapeutic scenario about emotional management for someone feeling {emotion}.\n",
    "    Difficulty: {difficulty}.\n",
    "    Format exactly as this JSON structure:\n",
    "    {{\n",
    "        \"scenario\": \"the scenario description\",\n",
    "        \"choices\": [\n",
    "            {{\"text\": \"choice 1\", \"correct\": false, \"explanation\": \"why this is wrong\"}},\n",
    "            {{\"text\": \"choice 2\", \"correct\": true, \"explanation\": \"why this is right\"}},\n",
    "            {{\"text\": \"choice 3\", \"correct\": false, \"explanation\": \"why this is wrong\"}},\n",
    "            {{\"text\": \"choice 4\", \"correct\": false, \"explanation\": \"why this is wrong\"}}\n",
    "        ],\n",
    "        \"difficulty\": \"{difficulty}\",\n",
    "        \"emotion\": \"{emotion}\"\n",
    "    }}\n",
    "    Important rules:\n",
    "    1. Make exactly 4 choices\n",
    "    2. Only one choice should be correct\n",
    "    3. Provide concrete explanations\n",
    "    4. Keep scenario realistic and therapeutic\n",
    "    5. Ensure explanations are psychologically valid\n",
    "    6. Make the correct choice non-obvious for harder difficulties\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"contents\": [{\n",
    "            \"parts\": [{\n",
    "                \"text\": prompt\n",
    "            }]\n",
    "        }],\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": 0.7 + (0.1 * [\"easy\", \"medium\", \"hard\"].index(difficulty))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{GEMINI_API_URL}?key={GEMINI_API_KEY}\",\n",
    "            headers=headers,\n",
    "            json=data\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        generated_text = response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        json_start = generated_text.find('{')\n",
    "        json_end = generated_text.rfind('}') + 1\n",
    "        json_str = generated_text[json_start:json_end]\n",
    "        \n",
    "        scenario = json.loads(json_str)\n",
    "        \n",
    "        # Validate scenario structure\n",
    "        if not all(k in scenario for k in [\"scenario\", \"choices\", \"difficulty\"]):\n",
    "            raise ValueError(\"Invalid scenario format\")\n",
    "        if len(scenario[\"choices\"]) != 4:\n",
    "            raise ValueError(\"Scenario must have exactly 4 choices\")\n",
    "            \n",
    "        return scenario\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating scenario: {str(e)}\")\n",
    "        # Return fallback scenario\n",
    "        return {\n",
    "            \"scenario\": f\"You're feeling {emotion} after a long day. How do you cope?\",\n",
    "            \"choices\": [\n",
    "                {\"text\": \"Bottle up your emotions\", \"correct\": False, \"explanation\": \"Suppressing emotions can lead to increased stress\"},\n",
    "                {\"text\": \"Talk to a trusted friend\", \"correct\": True, \"explanation\": \"Social support helps process emotions effectively\"},\n",
    "                {\"text\": \"Yell at someone nearby\", \"correct\": False, \"explanation\": \"This harms relationships and increases tension\"},\n",
    "                {\"text\": \"Ignore the feeling completely\", \"correct\": False, \"explanation\": \"Avoidance prevents emotional processing\"}\n",
    "            ],\n",
    "            \"difficulty\": difficulty,\n",
    "            \"emotion\": emotion\n",
    "        }\n",
    "\n",
    "class EmotionGameEnv(gym.Env):\n",
    "    \"\"\"Custom Gym environment for Emotion-Based RL with adaptive difficulty.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EmotionGameEnv, self).__init__()\n",
    "        \n",
    "        # Define state and action spaces\n",
    "        self.emotions = [\"sad\", \"angry\", \"happy\", \"stress\", \"neutral\", \"fear\", \"anxious\", \"overwhelmed\"]\n",
    "        self.difficulty_adjuster = DifficultyAdjuster()\n",
    "        self.action_space = spaces.Discrete(4)  # 4 choices per scenario\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"emotion\": spaces.Discrete(len(self.emotions)),\n",
    "            \"difficulty\": spaces.Discrete(len(self.difficulty_adjuster.difficulty_levels)),\n",
    "            \"history\": spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)  # Last 5 performances\n",
    "        })\n",
    "\n",
    "        self.state = None\n",
    "        self.current_scenario = None\n",
    "        self.performance_history = deque(maxlen=5)  # Tracks last 5 performances\n",
    "        self.stats = {\n",
    "            \"correct\": 0,\n",
    "            \"total\": 0,\n",
    "            \"difficulty_stats\": {d: {\"correct\": 0, \"total\": 0} for d in self.difficulty_adjuster.difficulty_levels},\n",
    "            \"emotion_stats\": {e: {\"correct\": 0, \"total\": 0} for e in self.emotions}\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment for a new episode.\"\"\"\n",
    "        emotion = random.choice(self.emotions)\n",
    "        difficulty = self.difficulty_adjuster.get_current_difficulty()\n",
    "        \n",
    "        self.state = {\n",
    "            \"emotion\": emotion,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"history\": np.array(list(self.performance_history) + [0] * (5 - len(self.performance_history)), dtype=np.float32)\n",
    "        }\n",
    "        \n",
    "        # Generate new scenario\n",
    "        self.current_scenario = generate_gemini_scenario(emotion, difficulty)\n",
    "        \n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Takes an action and returns (obs, reward, done, info).\"\"\"\n",
    "        if not self.current_scenario:\n",
    "            raise ValueError(\"No scenario loaded\")\n",
    "            \n",
    "        try:\n",
    "            chosen_choice = self.current_scenario[\"choices\"][action]\n",
    "            is_correct = chosen_choice[\"correct\"]\n",
    "            self.performance_history.append(1 if is_correct else 0)\n",
    "            \n",
    "            # Update difficulty based on performance\n",
    "            new_difficulty = self.difficulty_adjuster.update_difficulty(is_correct)\n",
    "            \n",
    "            # Calculate base reward\n",
    "            if is_correct:\n",
    "                reward = 10\n",
    "                self.stats[\"correct\"] += 1\n",
    "                self.stats[\"difficulty_stats\"][self.current_scenario[\"difficulty\"]][\"correct\"] += 1\n",
    "                self.stats[\"emotion_stats\"][self.current_scenario[\"emotion\"]][\"correct\"] += 1\n",
    "            else:\n",
    "                reward = -3\n",
    "                \n",
    "            self.stats[\"total\"] += 1\n",
    "            self.stats[\"difficulty_stats\"][self.current_scenario[\"difficulty\"]][\"total\"] += 1\n",
    "            self.stats[\"emotion_stats\"][self.current_scenario[\"emotion\"]][\"total\"] += 1\n",
    "            \n",
    "            # Apply difficulty multiplier\n",
    "            difficulty = self.current_scenario[\"difficulty\"]\n",
    "            if difficulty == \"medium\":\n",
    "                reward *= 1.5\n",
    "            elif difficulty == \"hard\":\n",
    "                reward *= 2\n",
    "                \n",
    "            # Add small reward for attempting harder difficulties\n",
    "            reward += [\"easy\", \"medium\", \"hard\"].index(difficulty) * 0.5\n",
    "                \n",
    "        except (IndexError, KeyError):\n",
    "            reward = -5  # Penalty for invalid action\n",
    "            is_correct = False\n",
    "            \n",
    "        done = True  # One step per episode\n",
    "        info = {\n",
    "            \"scenario\": self.current_scenario[\"scenario\"],\n",
    "            \"choices\": [c[\"text\"] for c in self.current_scenario[\"choices\"]],\n",
    "            \"chosen\": self.current_scenario[\"choices\"][action][\"text\"],\n",
    "            \"correct\": is_correct,\n",
    "            \"explanation\": chosen_choice[\"explanation\"],\n",
    "            \"difficulty\": self.current_scenario[\"difficulty\"],\n",
    "            \"emotion\": self.current_scenario[\"emotion\"],\n",
    "            \"stats\": self.stats,\n",
    "            \"new_difficulty\": new_difficulty\n",
    "        }\n",
    "        \n",
    "        return self._get_obs(), reward, done, info\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"Convert state to observation space format.\"\"\"\n",
    "        return {\n",
    "            \"emotion\": self.emotions.index(self.state[\"emotion\"]),\n",
    "            \"difficulty\": self.difficulty_adjuster.difficulty_levels.index(self.state[\"difficulty\"]),\n",
    "            \"history\": self.state[\"history\"]\n",
    "        }\n",
    "\n",
    "def train_rl_model(env, total_timesteps=10000, model_params=None):\n",
    "    \"\"\"Trains the RL model using Proximal Policy Optimization (PPO).\"\"\"\n",
    "    params = DEFAULT_MODEL_PARAMS.copy()\n",
    "    if model_params:\n",
    "        params.update(model_params)\n",
    "    \n",
    "    callback = TrainingCallback()\n",
    "    model = PPO(**params, env=env)\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nüéØ Training PPO model with adaptive difficulty...\\n\")\n",
    "        model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "        model.save(\"emotional_rl_agent_adaptive\")\n",
    "        print(f\"\\n‚úÖ Model training completed (Mean Reward: {callback.get_mean_reward():.2f})\")\n",
    "        print(f\"Saved as 'emotional_rl_agent_adaptive'\")\n",
    "        return model\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚è∏ Training interrupted. Saving model...\")\n",
    "        model.save(\"emotional_rl_agent_adaptive_interrupted\")\n",
    "        return model\n",
    "\n",
    "def evaluate_model(env, model=None, model_path=\"emotional_rl_agent_adaptive\", num_tests=10):\n",
    "    \"\"\"Evaluates the model with detailed output and adaptive difficulty.\"\"\"\n",
    "    if model is None:\n",
    "        print(\"\\nüì• Loading trained model...\")\n",
    "        model = PPO.load(model_path, env=env)\n",
    "\n",
    "    print(\"\\nüîç Starting Evaluation with Adaptive Difficulty...\")\n",
    "    for i in range(num_tests):\n",
    "        obs = env.reset()\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # Display results\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"üè∑Ô∏è  Test Case {i+1}/{num_tests}\")\n",
    "        print(f\"üé≠ Emotion: {info['emotion'].upper()}\")\n",
    "        print(f\"üìä Difficulty: {info['difficulty'].upper()} ‚Üí New: {info['new_difficulty'].upper()}\")\n",
    "        print(f\"\\nüìú Scenario: {info['scenario']}\")\n",
    "        \n",
    "        print(\"\\nüí° Choices:\")\n",
    "        for idx, choice in enumerate(info['choices']):\n",
    "            marker = \"‚úì\" if idx == action else \" \"\n",
    "            correctness = \"(CORRECT)\" if info['choices'][idx] == info['chosen'] and info['correct'] else \"\"\n",
    "            print(f\" {marker} {idx}. {choice} {correctness}\")\n",
    "        \n",
    "        print(f\"\\nü§ñ AI Chose: {action}. {info['chosen']}\")\n",
    "        print(f\"‚úÖ Correct: {'Yes' if info['correct'] else 'No'}\")\n",
    "        print(f\"üí° Explanation: {info['explanation']}\")\n",
    "        print(f\"üèÜ Reward: {reward:.1f}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "    # Print summary statistics\n",
    "    stats = info['stats']\n",
    "    accuracy = (stats[\"correct\"] / stats[\"total\"]) * 100 if stats[\"total\"] > 0 else 0\n",
    "    print(f\"\\nüìä Overall Accuracy: {accuracy:.1f}% ({stats['correct']}/{stats['total']})\")\n",
    "    \n",
    "    print(\"\\nüìà Difficulty Breakdown:\")\n",
    "    for diff in env.difficulty_adjuster.difficulty_levels:\n",
    "        diff_stats = stats[\"difficulty_stats\"][diff]\n",
    "        if diff_stats[\"total\"] > 0:\n",
    "            acc = (diff_stats[\"correct\"] / diff_stats[\"total\"]) * 100\n",
    "            print(f\"  {diff.upper()}: {acc:.1f}% ({diff_stats['correct']}/{diff_stats['total']})\")\n",
    "    \n",
    "    print(\"\\nüé≠ Emotion Performance:\")\n",
    "    for emotion in sorted(env.emotions):\n",
    "        emo_stats = stats[\"emotion_stats\"][emotion]\n",
    "        if emo_stats[\"total\"] > 0:\n",
    "            acc = (emo_stats[\"correct\"] / emo_stats[\"total\"]) * 100\n",
    "            print(f\"  {emotion.upper()}: {acc:.1f}% ({emo_stats['correct']}/{emo_stats['total']})\")\n",
    "\n",
    "def interactive_demo(env, model=None, model_path=\"emotional_rl_agent_adaptive\"):\n",
    "    \"\"\"Interactive demo where human can play against the AI.\"\"\"\n",
    "    if model is None:\n",
    "        print(\"\\nüì• Loading trained model...\")\n",
    "        model = PPO.load(model_path, env=env)\n",
    "\n",
    "    print(\"\\nüéÆ Interactive Mode - Compete against the AI!\")\n",
    "    print(\"You'll see the same scenarios and choices as the AI.\")\n",
    "    print(\"Try to get more correct answers than the AI!\\n\")\n",
    "    \n",
    "    num_rounds = int(input(\"Enter number of rounds (default 5): \") or 5)\n",
    "    human_score = 0\n",
    "    ai_score = 0\n",
    "\n",
    "    for round_num in range(1, num_rounds + 1):\n",
    "        obs = env.reset()\n",
    "        scenario = env.current_scenario\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"üîî Round {round_num}/{num_rounds}\")\n",
    "        print(f\"üé≠ Emotion: {scenario['emotion'].upper()}\")\n",
    "        print(f\"üìä Difficulty: {scenario['difficulty'].upper()}\")\n",
    "        print(f\"\\nüìú Scenario: {scenario['scenario']}\")\n",
    "\n",
    "        print(\"\\nüí° Choices:\")\n",
    "        for idx, choice in enumerate(scenario['choices']):\n",
    "            print(f\" {idx}. {choice['text']}\")\n",
    "\n",
    "        # Human choice\n",
    "        while True:\n",
    "            try:\n",
    "                human_action = int(input(\"\\nYour choice (0-3): \"))\n",
    "                if 0 <= human_action <= 3:\n",
    "                    break\n",
    "                print(\"Please enter a number between 0 and 3\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number\")\n",
    "\n",
    "        # AI choice\n",
    "        ai_action, _ = model.predict(obs)\n",
    "\n",
    "        # Evaluate both\n",
    "        _, human_reward, _, human_info = env.step(human_action)\n",
    "        _, ai_reward, _, ai_info = env.step(ai_action)\n",
    "\n",
    "        # Display results\n",
    "        print(f\"\\nü§ñ AI chose: {ai_action}. {ai_info['chosen']}\")\n",
    "        print(f\"üßë You chose: {human_action}. {human_info['chosen']}\")\n",
    "\n",
    "        print(f\"\\nüí° Correct answer: {[c['text'] for c in scenario['choices'] if c['correct']][0]}\")\n",
    "\n",
    "        if human_info['correct']:\n",
    "            human_score += 1\n",
    "            print(\"‚úÖ You got it right!\")\n",
    "        else:\n",
    "            print(f\"‚ùå Your explanation: {human_info['explanation']}\")\n",
    "\n",
    "        if ai_info['correct']:\n",
    "            ai_score += 1\n",
    "            print(\"‚úÖ AI got it right!\")\n",
    "        else:\n",
    "            print(f\"‚ùå AI explanation: {ai_info['explanation']}\")\n",
    "\n",
    "        print(f\"\\nüìä Score: You {human_score} - {ai_score} AI\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "    print(\"\\nüèÜ Final Results:\")\n",
    "    print(f\"You: {human_score} correct answers\")\n",
    "    print(f\"AI: {ai_score} correct answers\")\n",
    "\n",
    "    if human_score > ai_score:\n",
    "        print(\"\\nüéâ You beat the AI! Great job!\")\n",
    "    elif human_score == ai_score:\n",
    "        print(\"\\nü§ù It's a tie! Good match!\")\n",
    "    else:\n",
    "        print(\"\\nü§ñ The AI won this time. Try again!\")\n",
    "def main():\n",
    "    print(\"üöÄ Emotion-Based RL & AI Therapy System - Training Only\")\n",
    "    print(f\"üîë Using Gemini API Key: {GEMINI_API_KEY[:5]}...{GEMINI_API_KEY[-5:]}\")\n",
    "    \n",
    "    # Initialize environment\n",
    "    env = EmotionGameEnv()\n",
    "\n",
    "    # Train PPO model using default config\n",
    "    print(\"\\nüì¶ Training PPO model (default: 10000 timesteps)...\")\n",
    "    train_rl_model(env, total_timesteps=10000, model_params=DEFAULT_MODEL_PARAMS)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba603f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Emotion-Based RL & AI Therapy System - Training & Evaluation\n",
      "üîë Using Gemini API Key: AIzaS...wPI70\n",
      "\n",
      "üì¶ Training PPO model (default: 10000 timesteps)...\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "üéØ Training PPO model with adaptive difficulty...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\anaconda3\\envs\\whisper_env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating scenario: ('Connection aborted.', ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None))\n",
      "Error generating scenario: HTTPSConnectionPool(host='generativelanguage.googleapis.com', port=443): Max retries exceeded with url: /v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyA2rRBLcJx_e5g4d_fVOgG4q2Pf8ewPI70 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000181E58729A0>: Failed to resolve 'generativelanguage.googleapis.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error generating scenario: HTTPSConnectionPool(host='generativelanguage.googleapis.com', port=443): Max retries exceeded with url: /v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyA2rRBLcJx_e5g4d_fVOgG4q2Pf8ewPI70 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000181E5886340>: Failed to resolve 'generativelanguage.googleapis.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error generating scenario: HTTPSConnectionPool(host='generativelanguage.googleapis.com', port=443): Max retries exceeded with url: /v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyA2rRBLcJx_e5g4d_fVOgG4q2Pf8ewPI70 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000181E5872220>: Failed to resolve 'generativelanguage.googleapis.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error generating scenario: HTTPSConnectionPool(host='generativelanguage.googleapis.com', port=443): Max retries exceeded with url: /v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyA2rRBLcJx_e5g4d_fVOgG4q2Pf8ewPI70 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000181E584A3A0>: Failed to resolve 'generativelanguage.googleapis.com' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"üöÄ Emotion-Based RL & AI Therapy System - Training & Evaluation\")\n",
    "    print(f\"üîë Using Gemini API Key: {GEMINI_API_KEY[:5]}...{GEMINI_API_KEY[-5:]}\")\n",
    "\n",
    "    env = EmotionGameEnv()\n",
    "\n",
    "    print(\"\\nüì¶ Training PPO model (default: 10000 timesteps)...\")\n",
    "    model = train_rl_model(env, total_timesteps=10000, model_params=DEFAULT_MODEL_PARAMS)\n",
    "\n",
    "    print(\"\\nüß™ Evaluating Trained Model...\\n\")\n",
    "    evaluate_model(env, model=model, num_tests=5)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
