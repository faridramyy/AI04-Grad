{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from dotenv import load_dotenv\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI API Key securely from .env file\n",
    "def load_api_key():\n",
    "    env_path = \"../models/key.env\"\n",
    "    load_dotenv(env_path)  # Load API key from .env file\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"âŒ OpenAI API Key not found. Make sure it's in the .env file.\")\n",
    "    \n",
    "    openai.api_key = api_key\n",
    "    print(\"âœ… OpenAI API Key loaded successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_dataset():\n",
    "    \"\"\"Loads the JSON dataset for emotion-based scenarios.\"\"\"\n",
    "    dataset_path = \"../datasets/game.json\"\n",
    "\n",
    "    try:\n",
    "        with open(dataset_path, \"r\") as file:\n",
    "            dataset = json.load(file)\n",
    "        print(\"âœ… Dataset loaded successfully!\")\n",
    "        return dataset[\"intents\"]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: Dataset file '{dataset_path}' not found.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"âŒ Error: Failed to parse JSON file '{dataset_path}'.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Emotion-Based RL Environment\n",
    "class EmotionGameEnv(gym.Env):\n",
    "    \"\"\"Custom Gym environment for Emotion-Based RL.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EmotionGameEnv, self).__init__()\n",
    "\n",
    "        # Load emotion dataset\n",
    "        self.dataset = load_dataset()\n",
    "\n",
    "        # Define state and action spaces\n",
    "        self.emotions = [\"sad\", \"angry\", \"happy\", \"stress\", \"neutral\", \"fear\"]\n",
    "        self.action_space = spaces.Discrete(4)  # 4 choices per scenario\n",
    "        self.observation_space = spaces.Discrete(len(self.emotions))\n",
    "\n",
    "        self.state = None\n",
    "        self.current_scenario = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment for a new episode.\"\"\"\n",
    "        self.state = random.choice(self.emotions)  # Pick a random emotion\n",
    "        self.current_scenario = random.choice(self.dataset)  # Pick a random scenario\n",
    "        return self.emotions.index(self.state)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Takes an action, applies it to the environment, and gets a reward.\"\"\"\n",
    "        try:\n",
    "            responses = self.current_scenario[\"responses\"]\n",
    "            reward = responses[action][\"reward\"]\n",
    "        except (IndexError, KeyError):\n",
    "            reward = 0  # Default reward if action index is invalid\n",
    "        \n",
    "        done = True  # One step per episode\n",
    "        info = {\"scenario\": self.current_scenario[\"scenario\"]}\n",
    "        \n",
    "        return self.emotions.index(self.state), reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_response(emotion, scenario):\n",
    "    \"\"\"Generates a GPT-based response for an emotional scenario.\"\"\"\n",
    "    prompt = f\"The user is feeling {emotion}. The situation is: '{scenario}'. What should they do? Provide a short helpful response.\"\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o-mini\",  # Use gpt-4o-mini instead of gpt-4\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI therapist helping users manage emotions.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the RL Model\n",
    "def train_rl_model(env, total_timesteps=10000):\n",
    "    \"\"\"Trains the RL model using Proximal Policy Optimization (PPO).\"\"\"\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "    try:\n",
    "        print(\"\\nğŸ¯ Training PPO model...\\n\")\n",
    "        model.learn(total_timesteps=total_timesteps)\n",
    "        model.save(\"emotional_rl_agent\")\n",
    "        print(\"âœ… Model training completed and saved as 'emotional_rl_agent'.\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"â¸ Training interrupted. Saving model...\")\n",
    "        model.save(\"emotional_rl_agent\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Evaluate the Model\n",
    "def evaluate_model(env, model_path=\"emotional_rl_agent\", num_tests=10):\n",
    "    \"\"\"Loads a trained model and runs test cases.\"\"\"\n",
    "    print(\"\\nğŸ“¥ Loading trained model...\")\n",
    "    model = PPO.load(model_path)\n",
    "\n",
    "    obs = env.reset()\n",
    "    for i in range(num_tests):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # Get GPT-generated response\n",
    "        gpt_feedback = get_gpt_response(env.state, info[\"scenario\"])\n",
    "\n",
    "        # Display results\n",
    "        print(f\"\\nğŸ­ Emotion: {env.state}\")\n",
    "        print(f\"ğŸ“œ Scenario: {info['scenario']}\")\n",
    "        print(f\"âœ… Action Taken: {action}\")\n",
    "        print(f\"ğŸ† Reward: {reward}\")\n",
    "        print(f\"ğŸ’¬ AI Therapy Response: {gpt_feedback}\\n\")\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Emotion-Based RL & AI Therapy System...\n",
      "\n",
      "âœ… OpenAI API Key loaded successfully!\n",
      "âœ… Dataset loaded successfully!\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\anaconda3\\envs\\new_bert\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Training PPO model...\n",
      "\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | -0.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 1120     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.55        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 777         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026947517 |\n",
      "|    clip_fraction        | 0.756       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.00612    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.13        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.146      |\n",
      "|    value_loss           | 6.47        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 728         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044912666 |\n",
      "|    clip_fraction        | 0.816       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.000948    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.82        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.153      |\n",
      "|    value_loss           | 6.23        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 1.75       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 703        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09177713 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | 0.00176    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.65       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.128     |\n",
      "|    value_loss           | 5.48       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 2.22        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 693         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037239254 |\n",
      "|    clip_fraction        | 0.762       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.881      |\n",
      "|    explained_variance   | -0.00202    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.1         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.112      |\n",
      "|    value_loss           | 3.7         |\n",
      "-----------------------------------------\n",
      "âœ… Model training completed and saved as 'emotional_rl_agent'.\n",
      "\n",
      "ğŸ“¥ Loading trained model...\n",
      "\n",
      "ğŸ­ Emotion: fear\n",
      "ğŸ“œ Scenario: Itâ€™s a lazy Sunday, and you have no plans. You feel neither good nor bad. What do you do?\n",
      "âœ… Action Taken: 0\n",
      "ğŸ† Reward: 3\n",
      "ğŸ’¬ AI Therapy Response: Error: The model `gpt-4` does not exist or you do not have access to it.\n",
      "\n",
      "\n",
      "ğŸ­ Emotion: fear\n",
      "ğŸ“œ Scenario: You recently lost something or someone important to you. How do you cope?\n",
      "âœ… Action Taken: 0\n",
      "ğŸ† Reward: 3\n",
      "ğŸ’¬ AI Therapy Response: Error: The model `gpt-4` does not exist or you do not have access to it.\n",
      "\n",
      "\n",
      "ğŸ­ Emotion: stress\n",
      "ğŸ“œ Scenario: You had an argument with a friend, and you're feeling angry. What do you do?\n",
      "âœ… Action Taken: 2\n",
      "ğŸ† Reward: 2\n",
      "ğŸ’¬ AI Therapy Response: Error: The model `gpt-4` does not exist or you do not have access to it.\n",
      "\n",
      "\n",
      "ğŸ­ Emotion: sad\n",
      "ğŸ“œ Scenario: You just received great news! How do you celebrate?\n",
      "âœ… Action Taken: 0\n",
      "ğŸ† Reward: 3\n",
      "ğŸ’¬ AI Therapy Response: Error: The model `gpt-4` does not exist or you do not have access to it.\n",
      "\n",
      "\n",
      "ğŸ­ Emotion: stress\n",
      "ğŸ“œ Scenario: You accidentally break a family heirloom that holds sentimental value. How do you cope?\n",
      "âœ… Action Taken: 0\n",
      "ğŸ† Reward: 3\n",
      "ğŸ’¬ AI Therapy Response: Error: The model `gpt-4` does not exist or you do not have access to it.\n",
      "\n",
      "\n",
      "ğŸ­ Emotion: angry\n",
      "ğŸ“œ Scenario: You're stuck in heavy traffic, and it's making you angry. How do you react?\n",
      "âœ… Action Taken: 2\n",
      "ğŸ† Reward: 2\n",
      "ğŸ’¬ AI Therapy Response: Error: The model `gpt-4` does not exist or you do not have access to it.\n",
      "\n",
      "\n",
      "ğŸ­ Emotion: stress\n",
      "ğŸ“œ Scenario: You have a major deadline approaching, but you're feeling overwhelmed. What do you do?\n",
      "âœ… Action Taken: 0\n",
      "ğŸ† Reward: 2\n",
      "ğŸ’¬ AI Therapy Response: Error: The model `gpt-4` does not exist or you do not have access to it.\n",
      "\n",
      "\n",
      "ğŸ­ Emotion: neutral\n",
      "ğŸ“œ Scenario: You're at a social event, and you feel anxious. How do you handle it?\n",
      "âœ… Action Taken: 0\n",
      "ğŸ† Reward: 3\n",
      "ğŸ’¬ AI Therapy Response: Error: The model `gpt-4` does not exist or you do not have access to it.\n",
      "\n",
      "\n",
      "ğŸ­ Emotion: fear\n",
      "ğŸ“œ Scenario: You're stuck in heavy traffic, and it's making you angry. How do you react?\n",
      "âœ… Action Taken: 2\n",
      "ğŸ† Reward: 2\n",
      "ğŸ’¬ AI Therapy Response: Error: The model `gpt-4` does not exist or you do not have access to it.\n",
      "\n",
      "\n",
      "ğŸ­ Emotion: happy\n",
      "ğŸ“œ Scenario: You're stuck in heavy traffic, and it's making you angry. How do you react?\n",
      "âœ… Action Taken: 2\n",
      "ğŸ† Reward: 2\n",
      "ğŸ’¬ AI Therapy Response: Error: The model `gpt-4` does not exist or you do not have access to it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main Execution Function\n",
    "def main():\n",
    "    \"\"\"Main function to execute the complete RL training & GPT-based evaluation.\"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ Starting Emotion-Based RL & AI Therapy System...\\n\")\n",
    "    \n",
    "    # Load OpenAI API Key\n",
    "    load_api_key()\n",
    "\n",
    "    # Initialize the environment\n",
    "    env = EmotionGameEnv()\n",
    "\n",
    "    # Train the RL model\n",
    "    model = train_rl_model(env)\n",
    "\n",
    "    # Evaluate the trained model\n",
    "    evaluate_model(env)\n",
    "\n",
    "# Run the Main Function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Available models: ['dall-e-2', 'o1-mini-2024-09-12', 'o1-preview-2024-09-12', 'o1-mini', 'o1-preview', 'whisper-1', 'dall-e-3', 'babbage-002', 'omni-moderation-latest', 'omni-moderation-2024-09-26', 'tts-1-hd-1106', 'gpt-4o-mini', 'gpt-4o-mini-2024-07-18', 'tts-1-hd', 'tts-1', 'gpt-3.5-turbo-16k', 'tts-1-1106', 'davinci-002', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-instruct', 'gpt-3.5-turbo-instruct-0914', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo', 'text-embedding-3-large', 'text-embedding-3-small', 'text-embedding-ada-002']\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "try:\n",
    "    models = openai.Model.list()\n",
    "    available_models = [model[\"id\"] for model in models[\"data\"]]\n",
    "    print(\"âœ… Available models:\", available_models)\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error:\", str(e))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
