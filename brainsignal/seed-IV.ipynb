{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the enivironment i am using grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "\n",
    "# Define the parent directory for the dataset\n",
    "# parent_dir = r\"C:\\Users\\dell\\Desktop\\gradproject\\AI04-Grad\\datasets\\CREMA-D\\seed-IV\\eeg_raw_data\"\n",
    "parent_dir=\"C:\\graduation project folder\\datasets\\seed_iv\\eeg_raw_data\"\n",
    "# Specify the session to process (e.g., session 1)\n",
    "session = 1\n",
    "\n",
    "# Construct the path to the session directory\n",
    "session_path = os.path.join(parent_dir, str(session))\n",
    "\n",
    "# Check if the session directory exists\n",
    "if os.path.exists(session_path):\n",
    "    # List all observations (files) in the session directory\n",
    "    persons_list = os.listdir(session_path)\n",
    "    print(f\"Observations for session {session}: {persons_list}\")\n",
    "    \n",
    "    # Process each .mat file in the session directory\n",
    "    for person_file in persons_list:\n",
    "        if person_file.endswith('.mat'):  # Check for .mat files\n",
    "            file_path = os.path.join(session_path, person_file)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            # Load the .mat file\n",
    "            data = scipy.io.loadmat(file_path)\n",
    "            \n",
    "            # Print the keys in the .mat file to understand its structure\n",
    "            print(f\"Keys in {person_file}: {list(data.keys())}\")\n",
    "else:\n",
    "    print(f\"Session directory does not exist: {session_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "\n",
    "\n",
    "parent_dir=\"C:\\graduation project folder\\datasets\\seed_iv\\eeg_raw_data\"\n",
    "# Specify the session to process (e.g., session 1)\n",
    "session = 2\n",
    "\n",
    "# Construct the path to the session directory\n",
    "session_path = os.path.join(parent_dir, str(session))\n",
    "\n",
    "# Check if the session directory exists\n",
    "if os.path.exists(session_path):\n",
    "    # List all observations (files) in the session directory\n",
    "    persons_list = os.listdir(session_path)\n",
    "    print(f\"Observations for session {session}: {persons_list}\")\n",
    "    \n",
    "    # Process each .mat file in the session directory\n",
    "    for person_file in persons_list:\n",
    "        if person_file.endswith('.mat'):  # Check for .mat files\n",
    "            file_path = os.path.join(session_path, person_file)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            try:\n",
    "            # Attempt to load the .mat file\n",
    "                 data = scipy.io.loadmat(file_path)\n",
    "                 print(f\"Keys in {person_file}: {list(data.keys())}\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error processing file {file_path}: {e}\")\n",
    "else:\n",
    "    print(f\"Session directory does not exist: {session_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "\n",
    "\n",
    "parent_dir=\"C:\\graduation project folder\\datasets\\seed_iv\\eeg_raw_data\"\n",
    "# Specify the session to process (e.g., session 1)\n",
    "session = 3\n",
    "\n",
    "# Construct the path to the session directory\n",
    "session_path = os.path.join(parent_dir, str(session))\n",
    "\n",
    "# Check if the session directory exists\n",
    "if os.path.exists(session_path):\n",
    "    # List all observations (files) in the session directory\n",
    "    persons_list = os.listdir(session_path)\n",
    "    print(f\"Observations for session {session}: {persons_list}\")\n",
    "    \n",
    "    # Process each .mat file in the session directory\n",
    "    for person_file in persons_list:\n",
    "        if person_file.endswith('.mat'):  # Check for .mat files\n",
    "            file_path = os.path.join(session_path, person_file)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            try:\n",
    "            # Attempt to load the .mat file\n",
    "                data = scipy.io.loadmat(file_path)\n",
    "                print(f\"Keys in {person_file}: {list(data.keys())}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "else:\n",
    "    print(f\"Session directory does not exist: {session_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Base directory for EEG raw data\n",
    "base_dir = r\"C:\\graduation project folder\\datasets\\seed_iv\\eeg_raw_data\"\n",
    "target_base_dir = r\"C:\\graduation project folder\\datasets\\Final_SeedIV\"\n",
    "\n",
    "# Session mapping\n",
    "session_mapping = {\n",
    "    1: \"session1\",\n",
    "    2: \"session2\",\n",
    "    3: \"session3\"\n",
    "}\n",
    "\n",
    "# Move files to session folders\n",
    "for session, folder_name in session_mapping.items():\n",
    "    session_dir = os.path.join(target_base_dir, folder_name)\n",
    "    os.makedirs(session_dir, exist_ok=True)  # Create folder if not exists\n",
    "\n",
    "    session_source_dir = os.path.join(base_dir, str(session))\n",
    "    if os.path.exists(session_source_dir):\n",
    "        for file_name in os.listdir(session_source_dir):\n",
    "            if file_name.endswith(\".mat\"):\n",
    "                source_file = os.path.join(session_source_dir, file_name)\n",
    "                target_file = os.path.join(session_dir, file_name)\n",
    "                shutil.move(source_file, target_file)\n",
    "\n",
    "print(\"Files organized into session folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "# Base directory for organized sessions\n",
    "base_dir = r\"C:\\graduation project folder\\datasets\\Final_SeedIV\"\n",
    "sessions = [\"session1\", \"session2\", \"session3\"]\n",
    "\n",
    "# Initialize lists for data\n",
    "eeg_shapes = []\n",
    "eeg_ranges = []\n",
    "\n",
    "# Process each session\n",
    "for session in sessions:\n",
    "    session_dir = os.path.join(base_dir, session)\n",
    "    if not os.path.exists(session_dir):\n",
    "        print(f\"Session directory not found: {session_dir}\")\n",
    "        continue\n",
    "\n",
    "    for mat_file in os.listdir(session_dir):\n",
    "        if mat_file.endswith(\".mat\"):\n",
    "            file_path = os.path.join(session_dir, mat_file)\n",
    "            try:\n",
    "            # Attempt to load the .mat file\n",
    "                data = scipy.io.loadmat(file_path)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "            # Extract EEG data from keys\n",
    "            eeg_data = []\n",
    "            for key in data.keys():\n",
    "                if key.endswith(\"_eeg1\") or key.endswith(\"_eeg24\"):  # Adjust this logic based on keys\n",
    "                    eeg_data.append(data[key].flatten())\n",
    "\n",
    "            # Align and preprocess data\n",
    "            if len(eeg_data) > 0:\n",
    "                max_length = max(len(ch) for ch in eeg_data)\n",
    "                eeg_data_aligned = [\n",
    "                    np.pad(ch, (0, max_length - len(ch)), 'constant') if len(ch) < max_length else ch[:max_length]\n",
    "                    for ch in eeg_data\n",
    "                ]\n",
    "\n",
    "                eeg_data_aligned = np.stack(eeg_data_aligned, axis=0)\n",
    "                eeg_shapes.append(eeg_data_aligned.shape)\n",
    "                eeg_ranges.append((np.min(eeg_data_aligned), np.max(eeg_data_aligned)))\n",
    "\n",
    "# Output shapes and ranges for debugging\n",
    "print(\"EEG Data Shapes:\", eeg_shapes)\n",
    "print(\"EEG Data Ranges:\", eeg_ranges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example plot for EEG data from a single session\n",
    "eeg_sample = eeg_data_aligned[0]  # Take the first channel from aligned data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(eeg_sample)\n",
    "plt.title(\"EEG Signal from Channel 1\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sklearn\n",
    "\n",
    "print(f\"NumPy version: {numpy.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "import numpy as np\n",
    "\n",
    "def mi_matrix(X, N):\n",
    "    \"\"\"\n",
    "    Compute the mutual information matrix for EEG signals.\n",
    "    \n",
    "    Parameters:\n",
    "    X (array-like): EEG data with shape (channels, samples).\n",
    "    N (int): Number of EEG channels.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Mutual information matrix of shape (N, N).\n",
    "    \"\"\"\n",
    "    mat = np.zeros((N, N), dtype=float)\n",
    "    for i in range(N):\n",
    "        for j in range(i, N):  # Mutual information matrix is symmetric\n",
    "            mat[i][j] = mutual_info_score(X[i].ravel(), X[j].ravel())\n",
    "            mat[j][i] = mat[i][j]\n",
    "    return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_scalar_curvature(mi_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the Wasserstein scalar curvature for the MI matrix.\n",
    "\n",
    "    Parameters:\n",
    "    mi_matrix (np.ndarray): Mutual information matrix.\n",
    "\n",
    "    Returns:\n",
    "    float: Wasserstein scalar curvature.\n",
    "    \"\"\"\n",
    "    lambdas = np.linalg.eigvals(mi_matrix)\n",
    "    if np.any(lambdas <= 0):\n",
    "        return 0  # Avoid issues with negative eigenvalues\n",
    "\n",
    "    n = len(lambdas)\n",
    "    curvature = sum([1 / (lambdas[i] + lambdas[j]) for i in range(n) for j in range(n)])\n",
    "    return curvature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extracts the label from the filename based on a predefined mapping.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Name of the .mat file.\n",
    "\n",
    "    Returns:\n",
    "    int: Corresponding label ID.\n",
    "    \"\"\"\n",
    "    label_mapping = {\n",
    "        \"happy\": 0,\n",
    "        \"sad\": 1,\n",
    "        \"fear\": 2,\n",
    "        \"neutral\": 3\n",
    "    }\n",
    "    for label, label_id in label_mapping.items():\n",
    "        if label in filename.lower():\n",
    "            return label_id\n",
    "    raise ValueError(f\"Label not found in filename: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_eeg(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\graduation project folder\\datasets\\Final_SeedIV\\session1\\2_20150915.mat\"\n",
    "data = loadmat(file_path)\n",
    "print(data.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the session folder\n",
    "base_dir = r\"C:\\graduation project folder\\datasets\\Final_SeedIV\"\n",
    "sessions = [\"session1\", \"session2\", \"session3\"]\n",
    "\n",
    "# Function to process each session\n",
    "def process_session(session_dir):\n",
    "    for mat_file in os.listdir(session_dir):\n",
    "        if mat_file.endswith(\".mat\"):\n",
    "            file_path = os.path.join(session_dir, mat_file)\n",
    "            try:\n",
    "                data = loadmat(file_path)\n",
    "                print(f\"\\nProcessing file: {mat_file}\")\n",
    "                for key in data.keys():\n",
    "                    # Focus on keys ending with \"_eeg1\" to \"_eeg24\"\n",
    "                    if key.endswith(\"_eeg1\") or key.endswith(\"_eeg24\"):\n",
    "                        signal = data[key]\n",
    "                        print(f\"Key: {key}, Shape: {signal.shape}, Min: {np.min(signal)}, Max: {np.max(signal)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {mat_file}: {e}\")\n",
    "\n",
    "# Iterate through sessions\n",
    "for session in sessions:\n",
    "    session_dir = os.path.join(base_dir, session)\n",
    "    print(f\"\\n--- Session: {session} ---\")\n",
    "    process_session(session_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Define paths and session directories\n",
    "base_dir = r\"C:\\graduation project folder\\datasets\\Final_SeedIV\"\n",
    "sessions = [\"session1\", \"session2\", \"session3\"]\n",
    "\n",
    "# Adjusted parameters\n",
    "sampling_rate = 1000  # Replace with actual sampling rate if different\n",
    "segment_size = sampling_rate * 4  # 4 seconds window\n",
    "overlap = 0\n",
    "max_length = 2000000  # Optional truncation length\n",
    "\n",
    "eeg_data = []\n",
    "labels = []\n",
    "skipped_files = []  # To track files that were skipped due to errors\n",
    "file_segment_counts = []  # To track segments per file\n",
    "\n",
    "session_labels = {\n",
    "    \"session1\": [1, 2, 3, 0, 2, 0, 0, 1, 0, 1, 2, 1, 1, 1, 2],\n",
    "    \"session2\": [2, 1, 3, 0, 0, 2, 0, 2, 3, 3, 2, 3, 2, 0, 1],\n",
    "    \"session3\": [1, 2, 2, 1, 3, 3, 3, 1, 1, 2, 1, 0, 2, 3, 3]\n",
    "}\n",
    "\n",
    "# Helper function to sort files numerically\n",
    "def sort_files_numerically(file_list):\n",
    "    return sorted(file_list, key=lambda x: int(x.split('_')[0]))\n",
    "\n",
    "# Iterate through sessions\n",
    "for session in sessions:\n",
    "    print(f\"\\n--- Processing Session: {session} ---\")\n",
    "    session_dir = os.path.join(base_dir, session)\n",
    "    file_list = [f for f in os.listdir(session_dir) if f.endswith(\".mat\")]\n",
    "    file_list = sort_files_numerically(file_list)  # Sort files numerically\n",
    "    label_list = session_labels[session]\n",
    "\n",
    "    if len(file_list) != len(label_list):\n",
    "        raise ValueError(f\"Mismatch between files and labels in {session}\")\n",
    "\n",
    "    for idx, mat_file in enumerate(file_list):\n",
    "        try:\n",
    "            file_path = os.path.join(session_dir, mat_file)\n",
    "            print(f\"\\nProcessing file: {mat_file}\")\n",
    "            data = loadmat(file_path)  # Load the .mat file\n",
    "\n",
    "            eeg_signals = []\n",
    "            for key in data.keys():\n",
    "                # Look for keys ending in \"_eeg1\" to \"_eeg24\"\n",
    "                if key.endswith(tuple(f\"_eeg{i}\" for i in range(1, 25))):\n",
    "                    signal = data[key].flatten()\n",
    "\n",
    "                    # Debugging signal properties\n",
    "                    print(f\"Key: {key}, Length: {len(signal)}, Min: {signal.min()}, Max: {signal.max()}\")\n",
    "\n",
    "                    # Normalize signal\n",
    "                    signal = (signal - np.min(signal)) / (np.max(signal) - np.min(signal))\n",
    "\n",
    "                    # Pad short signals\n",
    "                    if len(signal) < segment_size:\n",
    "                        print(f\"Signal too short: {len(signal)}. Padding.\")\n",
    "                        signal = np.pad(signal, (0, segment_size - len(signal)), 'constant')\n",
    "\n",
    "                    # Truncate long signals\n",
    "                    if len(signal) > max_length:\n",
    "                        print(f\"Signal too long: {len(signal)}. Truncating to {max_length}.\")\n",
    "                        signal = signal[:max_length]\n",
    "\n",
    "                    # Segment into windows\n",
    "                    windows = [\n",
    "                        signal[i:i + segment_size]\n",
    "                        for i in range(0, len(signal) - segment_size + 1, segment_size - overlap)\n",
    "                    ]\n",
    "                    eeg_signals.append(windows)\n",
    "\n",
    "            if eeg_signals:\n",
    "                num_segments = min(len(channel) for channel in eeg_signals)\n",
    "                eeg_signals = [channel[:num_segments] for channel in eeg_signals]\n",
    "                eeg_signals_stacked = np.array(eeg_signals).transpose(1, 0, 2)\n",
    "                eeg_data.extend(eeg_signals_stacked)\n",
    "                labels.extend([label_list[idx]] * eeg_signals_stacked.shape[0])\n",
    "                file_segment_counts.append((mat_file, label_list[idx], eeg_signals_stacked.shape[0]))\n",
    "            else:\n",
    "                print(f\"No valid segments for file: {mat_file}\")\n",
    "                file_segment_counts.append((mat_file, label_list[idx], 0))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {mat_file} in {session}: {e}\")\n",
    "            skipped_files.append((session, mat_file))\n",
    "\n",
    "# Convert to arrays\n",
    "eeg_data = np.array(eeg_data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Debugging outputs\n",
    "print(\"\\n--- Debugging Outputs ---\")\n",
    "print(f\"Processed EEG Data Shape: {eeg_data.shape}\")\n",
    "print(f\"Processed Labels Shape: {labels.shape}\")\n",
    "\n",
    "# Count segments by label\n",
    "segment_counts = Counter(labels)\n",
    "print(\"\\nSegment Counts per Label:\", segment_counts)\n",
    "\n",
    "# File-level segment counts\n",
    "print(\"\\nSegments per File and Label:\")\n",
    "for file_name, label, segment_count in file_segment_counts:\n",
    "    print(f\"File: {file_name}, Label: {label}, Segments: {segment_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"EEG Data Shape: {eeg_data.shape}\")\n",
    "print(f\"Labels Shape: {labels.shape}\")\n",
    "print(f\"Unique Labels: {np.unique(labels)}\")\n",
    "print(f\"Segments per Label: {Counter(labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Segments with all zeros: {np.sum(np.all(eeg_data == 0, axis=(1, 2)))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the first channel of the first segment\n",
    "plt.plot(eeg_data[0, 0, :])\n",
    "plt.title(f\"Segment 0, Channel 0, Label: {labels[0]}\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    eeg_data, labels, test_size=0.20, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.20, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Display shapes\n",
    "print(f\"Training Data Shape: {X_train_final.shape}, Training Labels Shape: {y_train_final.shape}\")\n",
    "print(f\"Validation Data Shape: {X_val.shape}, Validation Labels Shape: {y_val.shape}\")\n",
    "print(f\"Testing Data Shape: {X_test.shape}, Testing Labels Shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=1000)  # Default threshold\n",
    "\n",
    "# Set NumPy to display the full array\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# Print the full `y_test` array\n",
    "print(f\"Testing Labels: {y_test}\")\n",
    "print(f\"training data: {X_val}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old model \n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(24, 4000, 1), kernel_regularizer=l2(0.01)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.3),  # Dropout for regularization\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.3),  # Dropout for regularization\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),  # Dropout for regularization\n",
    "    Dense(4, activation='softmax')  # 4 output classes\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Reshape data for CNN input (add channel dimension)\n",
    "X_train_cnn = X_train_final[..., np.newaxis]\n",
    "X_val_cnn = X_val[..., np.newaxis]\n",
    "X_test_cnn = X_test[..., np.newaxis]\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Simplified CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=(24, 4000, 1), kernel_regularizer=l2(0.01)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.4),  # Increased Dropout for better regularization\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.4),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),  # Reduced number of units\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')  # 4 output classes\n",
    "])\n",
    "\n",
    "# Compile the model with a lower learning rate for more stable training\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def hash_row(row):\n",
    "    return hashlib.md5(row.tobytes()).hexdigest()\n",
    "\n",
    "# Hash training and testing data\n",
    "train_hashes = {hash_row(row) for row in X_train.reshape(X_train.shape[0], -1)}\n",
    "test_hashes = {hash_row(row) for row in X_test.reshape(X_test.shape[0], -1)}\n",
    "\n",
    "# Check for overlap\n",
    "overlap = train_hashes.intersection(test_hashes)\n",
    "if overlap:\n",
    "    print(f\"Warning: Detected {len(overlap)} overlapping samples between train and test sets.\")\n",
    "else:\n",
    "    print(\"No overlap detected between train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_cnn, y_train_final,\n",
    "    validation_data=(X_val_cnn, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"eeg_cnn_model.trial with learning rate 0.1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(X_test_cnn, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy plot\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Loss plot\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "\n",
    "# Baseline Model\n",
    "baseline_model = DummyClassifier(strategy=\"uniform\")  # Random guesses\n",
    "baseline_model.fit(X_train_flattened, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "baseline_accuracy = baseline_model.score(X_test_flattened, y_test)\n",
    "print(f\"Baseline Model Test Accuracy (Random Guess): {baseline_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
    "print(X_train_flattened)\n",
    "\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_flattened, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train the SVM on the final training data\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Evaluate on validation data\n",
    "y_val_pred = svm_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"SVM Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "# Predict on the unseen test set\n",
    "y_test_pred = svm_model.predict(X_test_flattened)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"SVM Test Accuracy (Unseen Data): {test_accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bassem_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
