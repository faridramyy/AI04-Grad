{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepface pytesseract moviepy decord opencv-python matplotlib pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "# List of packages to check\n",
    "packages = ['deepface', 'pytesseract', 'moviepy', 'decord', 'opencv-python', 'matplotlib', 'pandas']\n",
    "\n",
    "# Get the versions of the installed packages\n",
    "installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set if pkg.key in packages}\n",
    "\n",
    "# Print installed packages and their versions\n",
    "for package, version in installed_packages.items():\n",
    "    print(f\"{package}: {version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytesseract\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepFace\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pytesseract\n",
    "from deepface import DeepFace\n",
    "import decord\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Set the path to the installed tesseract.exe (update this path based on your installation)\n",
    "pytesseract.pytesseract.tesseract_cmd =r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Directory path for your video dataset\n",
    "video_directory = r'C:\\Users\\dell\\Desktop\\graduation project\\AI04-Grad\\datasets\\CREMA-D\\crema-d videos'\n",
    "\n",
    "# Get all video file paths from the directory\n",
    "video_files = [os.path.join(video_directory, f) for f in os.listdir(video_directory) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "\n",
    "# Function to process videos and extract emotions and text from each frame\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_count = 0\n",
    "    results = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert frame to RGB (DeepFace expects RGB images)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Emotion Detection using DeepFace\n",
    "        try:\n",
    "            res = DeepFace.analyze(rgb_frame, actions=['emotion'], enforce_detection=False)[0]\n",
    "            emotion = {k: v for k, v in res['emotion'].items() if v > 70}\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing frame for emotions: {e}\")\n",
    "            emotion = {}\n",
    "\n",
    "        # Text Extraction using pytesseract\n",
    "        try:\n",
    "            text = pytesseract.image_to_string(frame)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from frame: {e}\")\n",
    "            text = \"\"\n",
    "\n",
    "        # Combine results for each frame\n",
    "        if len(emotion) > 0 or text.strip():\n",
    "            results.append({\n",
    "                'frame': frame_count,\n",
    "                'emotion': emotion,\n",
    "                'text': text.strip()\n",
    "            })\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return results\n",
    "\n",
    "# Function to display emotions and text over the video frames\n",
    "def display_results(results, video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_num = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Find corresponding result for the current frame\n",
    "        for result in results:\n",
    "            if result['frame'] == frame_num:\n",
    "                # Overlay detected emotions\n",
    "                if result['emotion']:\n",
    "                    emotion_text = ', '.join(f\"{k}: {v:.2f}%\" for k, v in result['emotion'].items())\n",
    "                    cv2.putText(frame, f\"Emotion: {emotion_text}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "                # Overlay extracted text\n",
    "                if result['text']:\n",
    "                    cv2.putText(frame, f\"Text: {result['text']}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "        # Display the video frame with annotations\n",
    "        cv2.imshow('Video with Emotions and Text', frame)\n",
    "\n",
    "        # Break on 'q' key press\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        frame_num += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Main Loop: Process each video in the directory\n",
    "for video in video_files:\n",
    "    print(f\"Processing video: {video}\")\n",
    "    video_results = process_video(video)\n",
    "\n",
    "    if video_results:\n",
    "        # Display results in the video\n",
    "        display_results(video_results, video)\n",
    "\n",
    "    print(f\"Finished processing {video}\")\n",
    "\n",
    "# Cleanup\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Running on GPU...\n",
      "Tensor on GPU:\n",
      "tensor([[-0.1634, -0.3398, -0.1766],\n",
      "        [ 2.1342,  0.5382, -0.0275],\n",
      "        [ 0.5097, -0.9536,  1.7265]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Verify if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! Running on GPU...\")\n",
    "    \n",
    "    # Create a random tensor and move it to the GPU\n",
    "    x = torch.randn(3, 3)\n",
    "    x = x.to('cuda')\n",
    "    print(\"Tensor on GPU:\")\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"CUDA not available. Running on CPU...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU...\n",
      "Matrix multiplication time: 0.35 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Running on the GPU...\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Running on the CPU...\")\n",
    "\n",
    "# Create two large tensors on the chosen device\n",
    "a = torch.randn(10000, 10000, device=device)\n",
    "b = torch.randn(10000, 10000, device=device)\n",
    "\n",
    "# Measure time for matrix multiplication\n",
    "start_time = time.time()\n",
    "c = torch.matmul(a, b)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Matrix multiplication time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pytesseract\n",
    "from deepface import DeepFace\n",
    "import concurrent.futures\n",
    "\n",
    "# Set the path to the installed tesseract.exe\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Directory path for your video dataset\n",
    "video_directory = r'C:\\Users\\dell\\Desktop\\graduation project\\AI04-Grad\\datasets\\CREMA-D\\crema-d videos'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get all video file paths from the directory\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m video_files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(video_directory, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(video_directory) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.avi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mov\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Get all video file paths from the directory\n",
    "video_files = [os.path.join(video_directory, f) for f in os.listdir(video_directory) if f.endswith(('.mp4', '.avi', '.mov'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "frame_skip = 5  # Process every nth frame\n",
    "resize_factor = 0.5  # Resize factor for frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_frame(frame):\n",
    "    # Resize frame\n",
    "    frame = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for face emotion recognition\n",
    "def recognize_emotion(frame):\n",
    "    # Convert frame to RGB (DeepFace expects RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Emotion Detection using DeepFace\n",
    "    try:\n",
    "        res = DeepFace.analyze(rgb_frame, actions=['emotion'], enforce_detection=False)[0]\n",
    "        emotion = {k: v for k, v in res['emotion'].items() if v > 70}\n",
    "        return emotion\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing frame for emotions: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for text recognition\n",
    "def recognize_text(frame):\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(frame)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from frame: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test and compare results\n",
    "def test_recognition(emotion, text):\n",
    "    # Example weights\n",
    "    emotion_weight = 0.6\n",
    "    text_weight = 0.4\n",
    "\n",
    "    # Determine dominant result\n",
    "    dominant_result = \"Emotion\" if emotion_weight > text_weight else \"Text\"\n",
    "    return dominant_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single video and collect results\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file {video_path}\")\n",
    "        return []\n",
    "\n",
    "    frame_count = 0\n",
    "    results = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Skip frames\n",
    "        if frame_count % frame_skip != 0:\n",
    "            frame_count += 1\n",
    "            continue\n",
    "\n",
    "        # Preprocess the frame\n",
    "        frame = preprocess_frame(frame)\n",
    "\n",
    "        # Get emotion and text\n",
    "        emotion = recognize_emotion(frame)\n",
    "        text = recognize_text(frame)\n",
    "\n",
    "        # Combine results for each frame\n",
    "        if len(emotion) > 0 or text:\n",
    "            results.append({\n",
    "                'frame': frame_count,\n",
    "                'emotion': emotion,\n",
    "                'text': text\n",
    "            })\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display results over the video frames\n",
    "def display_results(results, video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_num = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Find corresponding result for the current frame\n",
    "        for result in results:\n",
    "            if result['frame'] == frame_num:\n",
    "                # Display emotion and text\n",
    "                if result['emotion']:\n",
    "                    emotion_text = ', '.join(f\"{k}: {v:.2f}%\" for k, v in result['emotion'].items())\n",
    "                    cv2.putText(frame, f\"Emotion: {emotion_text}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "                if result['text']:\n",
    "                    cv2.putText(frame, f\"Text: {result['text']}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "        # Display the video frame with annotations\n",
    "        cv2.imshow('Video with Emotions and Text', frame)\n",
    "\n",
    "        # Break on 'q' key press\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        frame_num += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display results over the video frames\n",
    "def display_results(results, video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_num = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Find corresponding result for the current frame\n",
    "        for result in results:\n",
    "            if result['frame'] == frame_num:\n",
    "                # Display emotion and text\n",
    "                if result['emotion']:\n",
    "                    emotion_text = ', '.join(f\"{k}: {v:.2f}%\" for k, v in result['emotion'].items())\n",
    "                    cv2.putText(frame, f\"Emotion: {emotion_text}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "                if result['text']:\n",
    "                    cv2.putText(frame, f\"Text: {result['text']}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "        # Display the video frame with annotations\n",
    "        cv2.imshow('Video with Emotions and Text', frame)\n",
    "\n",
    "        # Break on 'q' key press, or add a slight delay\n",
    "        if cv2.waitKey(100) & 0xFF == ord('q'):  # Adjust wait time as necessary\n",
    "            break\n",
    "\n",
    "        frame_num += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "print(\"GPU device count: \", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Check if GPU is available\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA available: \u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "print(\"Number of GPUs: \", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current GPU: \", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without GPU: 4.1714217999997345\n",
      "with GPU: 1.3762896999996883\n"
     ]
    }
   ],
   "source": [
    "from numba import jit, cuda \n",
    "import numpy as np \n",
    "# to measure exec time \n",
    "from timeit import default_timer as timer \n",
    "\n",
    "# normal function to run on cpu \n",
    "def func(a):\t\t\t\t\t\t\t\t \n",
    "\tfor i in range(10000000): \n",
    "\t\ta[i]+= 1\t\n",
    "\n",
    "# function optimized to run on gpu \n",
    "@jit(target_backend='cuda')\t\t\t\t\t\t \n",
    "def func2(a): \n",
    "\tfor i in range(10000000): \n",
    "\t\ta[i]+= 1\n",
    "if __name__==\"__main__\": \n",
    "\tn = 10000000\t\t\t\t\t\t\t\n",
    "\ta = np.ones(n, dtype = np.float64) \n",
    "\t\n",
    "\tstart = timer() \n",
    "\tfunc(a) \n",
    "\tprint(\"without GPU:\", timer()-start)\t \n",
    "\t\n",
    "\tstart = timer() \n",
    "\tfunc2(a) \n",
    "\tprint(\"with GPU:\", timer()-start) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if the GPU is detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "No GPU detected.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# If CUDA is available, print additional GPU info\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepface'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepFace\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Verify if CUDA is available\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deepface'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "\n",
    "# Verify if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! Running on GPU...\")\n",
    "else:\n",
    "    print(\"CUDA not available. Running on CPU...\")\n",
    "\n",
    "# Load and analyze image\n",
    "image_path = \"example.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Run DeepFace analysis using GPU\n",
    "results = DeepFace.analyze(img_path=image_rgb, actions=['emotion'], enforce_detection=False, device='cuda')\n",
    "print(\"Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
