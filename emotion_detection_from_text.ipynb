{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_table(\n",
    "    \"./datasets/Emotion Detection from Text/train.txt\",\n",
    "    delimiter=\";\",\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "val = pd.read_table(\n",
    "    \"./datasets/Emotion Detection from Text/val.txt\",\n",
    "    delimiter=\";\",\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "test = pd.read_table(\n",
    "    \"./datasets/Emotion Detection from Text/test.txt\",\n",
    "    delimiter=\";\",\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "train.columns = [\"Text\", \"Emotion\"]\n",
    "val.columns = [\"Text\", \"Emotion\"]\n",
    "test.columns = [\"Text\", \"Emotion\"]\n",
    "\n",
    "# print(\"Train head:\", train.head())\n",
    "# print(\"Train shape:\", train.shape)\n",
    "# print(\"-\" * 10)\n",
    "# print(\"Val head:\", val.head())\n",
    "# print(\"Val shape:\", val.shape)\n",
    "# print(\"-\" * 10)\n",
    "# print(\"Test head:\", test.head())\n",
    "# print(\"Test shape:\", test.shape)\n",
    "# print(\"-\" * 10)\n",
    "\n",
    "data = pd.concat([train, val, test], ignore_index=True)\n",
    "data.columns = [\"text\", \"label\"]\n",
    "\n",
    "# print(\"Data head:\", data.head())\n",
    "print(\"Data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Function\n",
    "\n",
    "The preprocess function is designed to clean and normalize textual data by:\n",
    "\n",
    "1. **Removing unwanted characters**: Retaining only alphabetic characters.\n",
    "2. **Normalizing text**: Converting all characters to lowercase.\n",
    "3. **Tokenizing**: Splitting text into individual words.\n",
    "4. **Removing stopwords**: Eliminating common words that may not contribute to the analysis (e.g., \"and\", \"the\").\n",
    "5. **Stemming**: Reducing words to their root forms (e.g., \"running\" → \"run\").\n",
    "6. **Reconstructing the text**: Combining the processed words back into a single string.\n",
    "\n",
    "This process is essential in Natural Language Processing (NLP) tasks, such as emotion detection from text, to clean and prepare textual data for analysis or modeling.\n",
    "\n",
    "- **PorterStemmer**: A stemming algorithm provided by NLTK to reduce words to their base or root form.(e.g., \"connect\", \"connecting\", \"connected\" → \"connect\").\n",
    "- **stopwords**: A corpus in NLTK containing common words (like \"the\", \"is\", \"in\") that are typically removed in preprocessing because they carry minimal semantic value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def preprocess(line):\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \", line)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    # apply Stemming + remove the stopwords\n",
    "    review = [\n",
    "        PorterStemmer().stem(word)\n",
    "        for word in review\n",
    "        if not word in stopwords.words(\"english\")\n",
    "    ]\n",
    "\n",
    "    return \" \".join(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential Enhancements and Best Practices\n",
    "# Using Lemmatization Instead of Stemming:\n",
    "\n",
    "# Difference:\n",
    "\n",
    "# Stemming: Cuts off word suffixes to get to the root form (e.g., \"running\" → \"run\").\n",
    "# Lemmatization: Uses vocabulary and morphological analysis to return the base or dictionary form of a word (e.g., \"better\" → \"good\").\n",
    "# Advantage: Lemmatization tends to produce more meaningful roots.\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def preprocess(line):\n",
    "#     review = re.sub('[^a-zA-Z]', ' ', line)\n",
    "#     review = review.lower()\n",
    "#     review = review.split()\n",
    "#     review = [lemmatizer.lemmatize(word) for word in review if word not in stopwords.words('english')]\n",
    "#     return ' '.join(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "data[\"N_label\"] = label_encoder.fit_transform(data[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?????????????????????????????????????????????????????????????????????????????????????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=5000, ngram_range=(1, 3))\n",
    "data_cv = cv.fit_transform(data[\"text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_cv, data[\"N_label\"], test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape=(X_train.shape[1],), activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(6, activation=\"softmax\"))\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10)\n",
    "\n",
    "_, accuracy = model.evaluate(X_train, y_train)\n",
    "print(\"Accuracy: %.2f\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text = \"I feel sad\"\n",
    "text = preprocess(text)\n",
    "array = cv.transform([text]).toarray()\n",
    "pred = model.predict(array)\n",
    "a = np.argmax(pred, axis=1)\n",
    "label_encoder.inverse_transform(a)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "tf.keras.models.save_model(model, \"my_model.h5\")\n",
    "\n",
    "pickle.dump(label_encoder, open('encoder.pkl', 'wb'))\n",
    "pickle.dump(cv, open('CountVectorizer.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
